
device: cuda
C:\Users\User\anaconda3\Lib\site-packages\torch\nn\modules\rnn.py:878: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ..\aten\src\ATen\native\cudnn\RNN.cpp:982.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
(tensor([[ 0.0012,  0.0360, -0.0640,  ..., -0.0059, -0.0078,  0.0016],
        [ 0.0018,  0.0512, -0.0478,  ..., -0.0145, -0.0048, -0.0065],
        [ 0.0024,  0.0483, -0.0648,  ..., -0.0249, -0.0028, -0.0053],
        ...,
        [ 0.0254,  0.0408, -0.1085,  ..., -0.0302, -0.0097, -0.0153],
        [ 0.0317,  0.0552, -0.1032,  ..., -0.0198, -0.0217,  0.0039],
        [ 0.0572,  0.0554, -0.1323,  ..., -0.0186, -0.0264,  0.0264]],
       grad_fn=<SqueezeBackward1>), (tensor([[ 0.0572,  0.0554, -0.1323,  0.0450,  0.0560,  0.0394,  0.0216, -0.0076,
          0.0208,  0.0221, -0.0813, -0.0099, -0.0077,  0.0115, -0.0197,  0.0414,
         -0.0500,  0.0369,  0.0470, -0.0781, -0.0374, -0.0145, -0.0004, -0.0454,
         -0.0795, -0.0094,  0.0529,  0.2450, -0.1995, -0.0585, -0.0079,  0.0529,
         -0.0701,  0.0850, -0.0132,  0.0154,  0.0109, -0.0159, -0.0268, -0.0176,
          0.0201, -0.0051,  0.0568, -0.0214,  0.0607, -0.0063, -0.0240,  0.0206,
         -0.0378, -0.0180,  0.0370, -0.0313,  0.0088, -0.0565, -0.0010, -0.0814,
         -0.0733,  0.0196, -0.3396, -0.3355,  0.2680, -0.0000, -0.0908, -0.0122,
          0.0847, -0.0039,  0.0138, -0.0045,  0.0335, -0.0177, -0.0500, -0.0052,
         -0.0362, -0.0394,  0.0135, -0.0096, -0.0439,  0.0469, -0.0418, -0.0515,
         -0.0678,  0.0723, -0.0745,  0.0134,  0.0488, -0.0499, -0.0430, -0.0746,
         -0.0258, -0.0238,  0.0445, -0.0142, -0.0287, -0.0224, -0.0000, -0.0000,
          0.0570, -0.0186, -0.0264,  0.0264]], grad_fn=<SqueezeBackward1>), tensor([[ 1.0944e-01,  1.1258e-01, -2.7390e-01,  8.9347e-02,  1.1453e-01,
          7.7259e-02,  4.3021e-02, -1.3829e-02,  4.2801e-02,  4.5572e-02,
         -1.5038e-01, -2.0978e-02, -1.6929e-02,  2.3999e-02, -4.0541e-02,
          8.7843e-02, -1.0283e-01,  7.1256e-02,  9.2747e-02, -1.6855e-01,
         -7.4604e-02, -2.8971e-02, -7.7093e-04, -9.0486e-02, -1.6799e-01,
         -1.7273e-02,  1.0839e-01,  5.4228e-01, -4.5867e-01, -1.1527e-01,
         -1.6382e-02,  9.6651e-02, -1.3471e-01,  1.6718e-01, -2.7475e-02,
          3.2957e-02,  2.0881e-02, -3.3221e-02, -5.7302e-02, -3.5284e-02,
          4.1756e-02, -1.0460e-02,  1.0848e-01, -4.4413e-02,  1.3399e-01,
         -1.3036e-02, -4.7373e-02,  4.2178e-02, -7.7520e-02, -3.3918e-02,
          7.4754e-02, -6.8906e-02,  1.7145e-02, -1.1079e-01, -2.1162e-03,
         -1.5658e-01, -1.5876e-01,  4.0434e-02, -7.7266e-01, -7.4459e-01,
          5.4512e-01, -0.0000e+00, -1.7903e-01, -2.7425e-02,  1.7698e-01,
         -8.0834e-03,  2.7655e-02, -9.0680e-03,  6.7497e-02, -3.6128e-02,
         -1.0116e-01, -1.0405e-02, -7.2669e-02, -7.6549e-02,  2.7737e-02,
         -1.8722e-02, -9.2257e-02,  9.6214e-02, -7.7949e-02, -1.0721e-01,
         -1.5065e-01,  1.4043e-01, -1.5562e-01,  2.5444e-02,  1.0220e-01,
         -9.6388e-02, -8.5815e-02, -1.3655e-01, -5.0545e-02, -4.9613e-02,
          8.9162e-02, -2.7289e-02, -5.8257e-02, -4.5729e-02, -0.0000e+00,
         -0.0000e+00,  1.1806e-01, -3.7588e-02, -5.6814e-02,  5.3774e-02]],
       grad_fn=<SqueezeBackward1>)))
tensor([[-0.0315,  0.0281, -0.0210,  ...,  0.0155,  0.0042, -0.0169],
        [-0.0283,  0.0266, -0.0222,  ...,  0.0316,  0.0010, -0.0245],
        [-0.0160,  0.0175, -0.0163,  ...,  0.0320,  0.0076, -0.0302],
        ...,
        [-0.0719,  0.0332, -0.0344,  ...,  0.0531,  0.0029, -0.0525],
        [-0.0575,  0.0306, -0.0490,  ...,  0.0451,  0.0228, -0.0447],
        [-0.0651,  0.0237, -0.0300,  ...,  0.0425,  0.0306, -0.0395]],
       grad_fn=<SqueezeBackward1>)
torch.Size([128, 100])
torch.Size([128, 100])
tensor([[-0.0411],
        [-0.0784],
        [-0.0321],
        [-0.0026],
        [ 0.0039],
        [-0.0099],
        [-0.0226],
        [-0.0425],
        [-0.0532],
        [-0.0364],
        [-0.0403],
        [ 0.0298],
        [ 0.0099],
        [ 0.0442],
        [-0.0665],
        [-0.0384],
        [-0.0106],
        [ 0.0069],
        [-0.0390],
        [-0.0366],
        [-0.0224],
        [ 0.0148],
        [-0.0205],
        [-0.0794],
        [ 0.0216],
        [ 0.0282],
        [ 0.0402],
        [ 0.0039],
        [ 0.0693],
        [-0.0243],
        [-0.0271],
        [-0.0118],
        [ 0.0077],
        [ 0.0984],
        [-0.0398],
        [-0.0198],
        [-0.0510],
        [ 0.0033],
        [-0.0136],
        [-0.0181],
        [ 0.0424],
        [ 0.0476],
        [-0.0700],
        [-0.0663],
        [ 0.0302],
        [-0.0590],
        [ 0.0070],
        [-0.0329],
        [ 0.0191],
        [ 0.0108],
        [ 0.0258],
        [-0.0007],
        [-0.0468],
        [-0.0227],
        [-0.0233],
        [-0.0101],
        [-0.0416],
        [-0.0127],
        [ 0.0027],
        [ 0.0319],
        [-0.0151],
        [-0.0209],
        [-0.0542],
        [-0.0726],
        [ 0.0927],
        [-0.0266],
        [-0.0013],
        [-0.0216],
        [ 0.0205],
        [-0.0129],
        [-0.0356],
        [-0.0317],
        [-0.0086],
        [ 0.0125],
        [-0.0057],
        [-0.0194],
        [-0.0049],
        [-0.0203],
        [ 0.0031],
        [ 0.0106],
        [-0.0175],
        [ 0.0155],
        [-0.0129],
        [-0.0618],
        [-0.0432],
        [ 0.0113],
        [ 0.0410],
        [-0.0252],
        [-0.0098],
        [ 0.0239],
        [-0.0512],
        [-0.0145],
        [-0.0233],
        [-0.0366],
        [-0.0555],
        [-0.0365],
        [-0.0234],
        [-0.0062],
        [-0.0362],
        [-0.0306],
        [ 0.0407],
        [ 0.0267],
        [-0.0267],
        [ 0.0125],
        [-0.0160],
        [ 0.0034],
        [-0.0480],
        [-0.0244],
        [-0.0163],
        [-0.0124],
        [-0.0273],
        [ 0.0535],
        [-0.0364],
        [-0.0632],
        [-0.0345],
        [-0.0585],
        [-0.0615],
        [-0.0562],
        [ 0.0142],
        [-0.0510],
        [-0.0875],
        [ 0.0609],
        [-0.0314],
        [ 0.0361],
        [ 0.0717],
        [-0.0060],
        [-0.0107],
