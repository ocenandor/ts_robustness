
device: cuda
C:\Users\User\anaconda3\Lib\site-packages\torch\nn\modules\rnn.py:878: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ..\aten\src\ATen\native\cudnn\RNN.cpp:982.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
tensor([[ 0.3335, -0.1028, -0.1146,  ...,  0.4900,  0.2439, -0.0042],
        [ 0.0510,  0.0267,  0.3116,  ..., -0.3456,  0.2550,  0.1017],
        [ 0.1857,  0.2255, -0.0111,  ..., -0.3125, -0.0171, -0.0016],
        ...,
        [-0.0288, -0.0477, -0.1125,  ...,  0.0624,  0.2345, -0.0338],
        [ 0.0024, -0.1397, -0.1165,  ...,  0.4170,  0.1266, -0.0192],
        [ 0.1697, -0.1581, -0.3656,  ...,  0.1411,  0.0476,  0.0231]],
       device='cuda:0', grad_fn=<SqueezeBackward1>)
tensor([[0.5302],
        [0.5401],
        [0.5337],
        [0.5328],
        [0.5290],
        [0.5247],
        [0.5289],
        [0.5421],
        [0.5259],
        [0.5337],
        [0.5249],
        [0.5366],
        [0.5299],
        [0.5197],
        [0.5345],
        [0.5382],
        [0.5436],
        [0.5450],
        [0.5336],
        [0.5404],
        [0.5432],
        [0.5280],
        [0.5313],
        [0.5490],
        [0.5333],
        [0.5431],
        [0.5327],
        [0.5382],
        [0.5419],
        [0.5303],
        [0.5447],
        [0.5458],
        [0.5276],
        [0.5479],
        [0.5357],
        [0.5389],
        [0.5402],
        [0.5333],
        [0.5351],
        [0.5248],
        [0.5391],
        [0.5349],
        [0.5388],
        [0.5343],
        [0.5300],
        [0.5445],
        [0.5226],
        [0.5395],
        [0.5376],
        [0.5381],
        [0.5391],
        [0.5371],
        [0.5420],
        [0.5433],
        [0.5480],
        [0.5310],
        [0.5453],
        [0.5429],
        [0.5464],
        [0.5405],
        [0.5458],
        [0.5314],
        [0.5260],
        [0.5332],
        [0.5393],
        [0.5345],
        [0.5282],
        [0.5305],
        [0.5384],
        [0.5476],
        [0.5320],
        [0.5384],
        [0.5311],
        [0.5274],
        [0.5432],
        [0.5165],
        [0.5317],
        [0.5362],
        [0.5408],
        [0.5207],
        [0.5320],
        [0.5342],
        [0.5320],
        [0.5462],
        [0.5321],
        [0.5489],
        [0.5303],
        [0.5426],
        [0.5346],
        [0.5424],
        [0.5345],
        [0.5405],
        [0.5410],
        [0.5441],
        [0.5262],
        [0.5408],
        [0.5455],
        [0.5350],
        [0.5377],
        [0.5407],
        [0.5301],
        [0.5415],
        [0.5350],
        [0.5354],
        [0.5296],
        [0.5448],
        [0.5457],
        [0.5507],
        [0.5309],
        [0.5397],
        [0.5470],
        [0.5356],
        [0.5261],
        [0.5316],
        [0.5164],
        [0.5406],
        [0.5352],
        [0.5326],
        [0.5404],
        [0.5461],
        [0.5309],
        [0.5313],
        [0.5370],
        [0.5396],
        [0.5363],
        [0.5301],
        [0.5334],
        [0.5272]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ToCopyBackward0>)