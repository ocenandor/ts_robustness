{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from functools import partial\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import tqdm\n",
    "import wandb\n",
    "from ignite.contrib.handlers import wandb_logger\n",
    "from ignite.engine import (Engine, Events, create_supervised_evaluator,\n",
    "                           create_supervised_trainer)\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.handlers.param_scheduler import LRScheduler\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "\n",
    "sys.path.append('../')\n",
    "from attacks.deepfool import deepfool\n",
    "from src.datasets import FordDataset\n",
    "from src.models import TransformerClassification\n",
    "from src.utils import build_optimizer, str2torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../configs/transformer_87.json') as f:\n",
    "    config =  json.load(f)\n",
    "config['train']['optimizer'] = str2torch(config['train']['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../data/FordA/FordA_TEST.arff\"\n",
    "test_dataset = FordDataset(test_path, config['data'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = TransformerClassification(config).to(device)\n",
    "weights = torch.load(r'C:\\Users\\ptmeg\\OneDrive\\Документы\\Skoltech\\Term3\\ML\\ts_robustness\\models\\best_model_29_accuracy=0.8868.pt')\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        y_pred = model(x)\n",
    "        return y_pred, y\n",
    "\n",
    "test_evaluator = Engine(validation_step)\n",
    "\n",
    "# Attach metrics to the evaluators\n",
    "metrics = {\n",
    "    'accuracy': Accuracy(output_transform=lambda x: (torch.argmax(x[0], dim=1), x[1])),\n",
    "}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(test_evaluator, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.884185606060606"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.run(test_dataloader)\n",
    "test_evaluator.state.metrics['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/10560 [00:00<02:54, 60.24it/s]\n"
     ]
    }
   ],
   "source": [
    "iters = []\n",
    "for i in tqdm.tqdm(range(len(test_dataset))):\n",
    "    if i == 20: break\n",
    "    test_sample = torch.from_numpy(test_dataset[i][0]).unsqueeze(1).to(device)\n",
    "    r_tot, loop_i, label, k_i, pert_image = deepfool(test_sample, model, 2, 0.2, max_iter=30, device=device)\n",
    "    iters.append(loop_i)\n",
    "    # if i == 100:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(iters, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4]), array([6, 8, 3, 3], dtype=int64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.15, 1.0136567466356647)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(iters), np.std(iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_robustness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
