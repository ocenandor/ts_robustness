{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from functools import partial\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import tqdm\n",
    "import wandb\n",
    "from ignite.contrib.handlers import wandb_logger\n",
    "from ignite.engine import (Engine, Events, create_supervised_evaluator,\n",
    "                           create_supervised_trainer)\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.handlers.param_scheduler import LRScheduler\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "\n",
    "sys.path.append('../')\n",
    "from attacks.deepfool import deepfool\n",
    "from src.datasets import FordDataset\n",
    "from src.models import TransformerClassification\n",
    "from src.utils import build_optimizer, str2torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../configs/transformer_87.json') as f:\n",
    "    config =  json.load(f)\n",
    "config['train']['optimizer'] = str2torch(config['train']['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../data/FordA/FordA_TEST.arff\"\n",
    "test_dataset = FordDataset(test_path, config['data'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = TransformerClassification(config).to(device)\n",
    "weights = torch.load('../models/trans_2outp_87.pth')\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        y_pred = model(x)\n",
    "        return y_pred, y\n",
    "\n",
    "test_evaluator = Engine(validation_step)\n",
    "\n",
    "# Attach metrics to the evaluators\n",
    "metrics = {\n",
    "    'accuracy': Accuracy(output_transform=lambda x: (torch.argmax(x[0], dim=1), x[1])),\n",
    "}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(test_evaluator, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptmeg\\miniconda3\\envs\\ts_robustness\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8616477272727273"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator.run(test_dataloader)\n",
    "test_evaluator.state.metrics['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfool(signal, model, num_classes=10, overshoot=0.02, max_iter=50, device='cpu'):\n",
    "\n",
    "    \"\"\"\n",
    "       :param signal: signal of size L x 1\n",
    "       :param model: modelwork (input: signals, output: values of activation **BEFORE** softmax).\n",
    "       :param num_classes: num_classes (limits the number of classes to test against, by default = 10)\n",
    "       :param overshoot: used as a termination criterion to prevent vanishing updates (default = 0.02).\n",
    "       :param max_iter: maximum number of iterations for deepfool (default = 50)\n",
    "       :return: minimal perturbation that fools the classifier, number of iterations that it required, new estimated_label and perturbed signal\n",
    "    \"\"\"\n",
    "\n",
    "    f_signal = model(signal).detach().cpu().numpy().flatten()\n",
    "    I = (np.array(f_signal)).flatten().argsort()[::-1]\n",
    "\n",
    "    I = I[0:num_classes]\n",
    "    label = I[0]\n",
    "\n",
    "    input_shape = signal.cpu().numpy().shape\n",
    "    pert_signal = copy.deepcopy(signal)\n",
    "    w = np.zeros(input_shape)\n",
    "    r_tot = np.zeros(input_shape)\n",
    "\n",
    "    loop_i = 0\n",
    "\n",
    "    x = Variable(pert_signal, requires_grad=True)\n",
    "    fs = model(x)\n",
    "    k_i = label\n",
    "\n",
    "    while k_i == label and loop_i < max_iter:\n",
    "\n",
    "        pert = np.inf\n",
    "        fs[0, I[0]].backward(retain_graph=True)\n",
    "        grad_orig = x.grad.data.cpu().numpy().copy()\n",
    "\n",
    "        for k in range(1, num_classes):\n",
    "            if x.grad is not None:\n",
    "                x.grad.zero_()\n",
    "\n",
    "            fs[0, I[k]].backward(retain_graph=True)\n",
    "            cur_grad = x.grad.data.cpu().numpy().copy()\n",
    "\n",
    "            # set new w_k and new f_k\n",
    "            w_k = cur_grad - grad_orig\n",
    "            f_k = (fs[0, I[k]] - fs[0, I[0]]).data.cpu().numpy()\n",
    "\n",
    "            pert_k = abs(f_k)/(np.linalg.norm(w_k.flatten())  + 1e-10)\n",
    "\n",
    "            # determine which w_k to use\n",
    "            if pert_k < pert:\n",
    "                pert = pert_k\n",
    "                w = w_k\n",
    "\n",
    "        # compute r_i and r_tot\n",
    "        # Added 1e-4 for numerical stability\n",
    "        r_i =  (pert+1e-4) * w / (np.linalg.norm(w) + 1e-10)\n",
    "        r_tot = np.float32(r_tot + r_i)\n",
    "\n",
    "        pert_signal = signal + (1+overshoot)*torch.from_numpy(r_tot).to(device)\n",
    "\n",
    "        x = Variable(pert_signal, requires_grad=True)\n",
    "        fs = model.forward(x)\n",
    "        k_i = np.argmax(fs.data.cpu().numpy().flatten())\n",
    "\n",
    "        loop_i += 1\n",
    "\n",
    "    r_tot = (1+overshoot)*r_tot\n",
    "\n",
    "    return r_tot, loop_i, label, k_i, pert_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10560/10560 [04:13<00:00, 41.73it/s]\n"
     ]
    }
   ],
   "source": [
    "iters = []\n",
    "for i in tqdm.tqdm(range(len(test_dataset))):\n",
    "    test_sample = torch.from_numpy(test_dataset[i][0]).unsqueeze(0).to(device)\n",
    "    r_tot, loop_i, label, k_i, pert_image = deepfool(test_sample, model, 2, 0.2, max_iter=30, device=device)\n",
    "    iters.append(loop_i)\n",
    "    # if i == 100:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(iters, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8, 30]),\n",
       " array([4770, 3299, 1125,  269,   73,   19,    8,    2,  995], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.869223484848485, 8.161662027340961)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(iters), np.std(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(r'C:\\Users\\ptmeg\\OneDrive\\Документы\\Skoltech\\Term3\\ML\\ts_robustness\\wandb\\run-20240311_215216-udqdkcx2\\saved_models\\best_model_5_accuracy=0.8653.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding_layer.weight',\n",
       "              tensor([[[-0.6251,  0.3863,  0.2842]],\n",
       "              \n",
       "                      [[ 0.1170, -0.4436,  0.1889]],\n",
       "              \n",
       "                      [[ 0.2122, -0.4185,  0.0942]],\n",
       "              \n",
       "                      [[-0.2163, -0.4803, -0.1294]],\n",
       "              \n",
       "                      [[ 0.3517, -0.1285,  0.0312]],\n",
       "              \n",
       "                      [[-0.3919, -0.3706,  0.1335]],\n",
       "              \n",
       "                      [[ 0.4829,  0.2424, -0.2752]],\n",
       "              \n",
       "                      [[-0.3305,  0.0268, -0.2034]],\n",
       "              \n",
       "                      [[-0.0890,  0.4211,  0.3742]],\n",
       "              \n",
       "                      [[-0.2234, -0.0377,  0.4886]],\n",
       "              \n",
       "                      [[ 0.1856,  0.4476, -0.0331]],\n",
       "              \n",
       "                      [[ 0.3207, -0.0376, -0.4554]],\n",
       "              \n",
       "                      [[-0.1824, -0.2432,  0.2769]],\n",
       "              \n",
       "                      [[-0.3588, -0.2687,  0.1051]],\n",
       "              \n",
       "                      [[ 0.3238, -0.4774,  0.4665]],\n",
       "              \n",
       "                      [[-0.4141, -0.4362, -0.3616]],\n",
       "              \n",
       "                      [[-0.3583,  0.1063,  0.2168]],\n",
       "              \n",
       "                      [[ 0.4267, -0.3919, -0.0289]],\n",
       "              \n",
       "                      [[ 0.4572, -0.1636,  0.1897]],\n",
       "              \n",
       "                      [[-0.0369,  0.4981,  0.3936]],\n",
       "              \n",
       "                      [[-0.5461, -0.1149,  0.2706]],\n",
       "              \n",
       "                      [[ 0.0960,  0.4564,  0.0085]],\n",
       "              \n",
       "                      [[-0.3545, -0.4218,  0.5036]],\n",
       "              \n",
       "                      [[ 0.4673, -0.3103, -0.2636]],\n",
       "              \n",
       "                      [[ 0.2123, -0.3802, -0.0455]]], device='cuda:0')),\n",
       "             ('embedding_layer.bias',\n",
       "              tensor([-0.3190, -0.4502, -0.0717, -0.5073,  0.0226, -0.4719, -0.1801, -0.5050,\n",
       "                      -0.0700,  0.3465, -0.0084, -0.1724, -0.3053,  0.2084, -0.5142,  0.4061,\n",
       "                       0.5100, -0.3206, -0.5155,  0.0402,  0.1379, -0.4218,  0.3834, -0.2609,\n",
       "                       0.2958], device='cuda:0')),\n",
       "             ('encoder_layer.self_attn.in_proj_weight',\n",
       "              tensor([[-0.1470, -0.1773,  0.0928,  ...,  0.0214, -0.0298, -0.0527],\n",
       "                      [-0.1628, -0.0349, -0.1385,  ..., -0.2170, -0.0616, -0.0957],\n",
       "                      [ 0.2183, -0.1252, -0.1000,  ..., -0.2213,  0.0112, -0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0040, -0.0783,  0.0478,  ...,  0.1641, -0.0736, -0.1752],\n",
       "                      [ 0.1025, -0.1492,  0.0401,  ..., -0.1621,  0.1613, -0.0442],\n",
       "                      [ 0.1895,  0.0004,  0.0585,  ...,  0.0481,  0.0039,  0.0810]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layer.self_attn.in_proj_bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0.], device='cuda:0')),\n",
       "             ('encoder_layer.self_attn.out_proj.weight',\n",
       "              tensor([[ 6.1020e-02, -7.5443e-02, -8.3340e-02, -7.1025e-02, -8.6696e-02,\n",
       "                       -1.8834e-01,  1.8343e-01,  1.7674e-01,  1.1509e-01, -1.7052e-01,\n",
       "                        8.6715e-02, -1.3342e-01,  1.5598e-01,  1.3693e-01, -1.5790e-01,\n",
       "                        1.4428e-01,  9.3185e-02, -1.4453e-01, -1.0998e-01, -8.0894e-02,\n",
       "                       -1.5472e-01, -1.4395e-01,  5.6855e-02,  1.3025e-02, -3.2870e-02],\n",
       "                      [-1.2810e-01, -7.9868e-03, -1.3175e-01, -1.9296e-01, -2.0339e-02,\n",
       "                       -1.7469e-01, -5.1298e-03, -1.7806e-01, -1.3456e-01, -6.7005e-02,\n",
       "                       -1.2941e-01,  6.7233e-02, -1.4602e-01, -2.6966e-02,  1.5535e-02,\n",
       "                       -1.1939e-01,  1.2459e-01,  1.8930e-01, -7.2227e-02, -1.2389e-02,\n",
       "                        1.2677e-01,  2.6175e-03, -1.9382e-01, -4.1476e-03,  1.3764e-02],\n",
       "                      [-8.8223e-02, -4.5378e-02,  1.1053e-01, -1.8293e-01,  1.4121e-01,\n",
       "                       -1.7406e-01,  1.3383e-01,  6.7606e-02,  5.3562e-02, -7.8259e-02,\n",
       "                       -1.2548e-01,  1.3201e-01, -1.3044e-01,  1.0323e-01, -1.8751e-01,\n",
       "                        1.2052e-01,  1.7562e-03, -1.5215e-01,  1.9868e-01,  4.9004e-02,\n",
       "                        6.1875e-02,  8.4720e-02, -9.1635e-02, -1.7283e-01, -9.1436e-02],\n",
       "                      [-3.8500e-02,  1.3028e-01,  4.2806e-02, -3.3024e-02, -1.2283e-01,\n",
       "                        1.4722e-01, -1.4218e-01,  7.3606e-02,  4.4403e-03,  1.7098e-01,\n",
       "                        1.5695e-01,  4.9655e-03,  5.4722e-02,  8.0412e-02,  1.7514e-01,\n",
       "                       -5.6154e-03, -1.3209e-01,  3.0452e-02,  7.6673e-02, -1.1008e-02,\n",
       "                       -3.3587e-02,  4.2419e-03, -2.1504e-02, -1.7558e-01, -1.1995e-01],\n",
       "                      [ 1.2498e-01, -1.2706e-02, -1.6383e-02, -1.8121e-01,  1.6266e-01,\n",
       "                       -1.9396e-02, -1.1942e-01, -1.2078e-01,  1.9984e-01,  1.3738e-01,\n",
       "                        1.4427e-01,  7.2118e-02,  1.1854e-01,  1.4715e-01,  6.8628e-02,\n",
       "                        1.1802e-01, -1.4878e-01, -1.9601e-01, -1.8760e-01, -1.8933e-01,\n",
       "                        2.0177e-02,  1.9921e-01, -7.9780e-02, -1.2915e-01,  1.4393e-01],\n",
       "                      [ 3.0588e-02,  2.4449e-02,  1.7627e-01,  7.4872e-02, -1.7999e-01,\n",
       "                       -9.9064e-02,  2.3534e-02,  7.8725e-02,  1.4003e-01,  1.4090e-01,\n",
       "                       -1.5581e-01, -8.8146e-02, -1.1277e-01, -1.1647e-01, -1.2933e-01,\n",
       "                        2.8260e-02, -2.5329e-02,  7.3719e-04, -1.0293e-01, -1.9162e-01,\n",
       "                        1.0498e-01, -1.7190e-02,  1.9130e-02, -1.8633e-01, -1.2626e-01],\n",
       "                      [ 1.7795e-01, -4.8690e-02,  6.9072e-02,  1.5809e-01,  1.3183e-01,\n",
       "                       -1.4110e-01,  1.7978e-01,  1.6137e-01,  4.0909e-02,  1.1781e-01,\n",
       "                       -1.3103e-01,  1.8567e-01,  1.2370e-01,  1.0797e-01, -3.7988e-02,\n",
       "                       -9.0170e-02, -3.3705e-02, -1.9133e-01, -1.3204e-01, -1.6257e-01,\n",
       "                        9.1498e-02, -5.2078e-02,  6.3985e-02,  4.4684e-02, -5.8922e-02],\n",
       "                      [-1.8030e-01,  1.7272e-01, -1.2426e-01, -1.9962e-01, -1.7085e-01,\n",
       "                        1.0488e-01,  1.6663e-01, -6.1224e-02,  1.0921e-01, -1.7453e-01,\n",
       "                        1.9843e-01,  8.0933e-02,  8.7150e-02, -1.1858e-02,  1.2452e-01,\n",
       "                       -8.6258e-02,  1.7664e-01, -1.4575e-01, -2.9418e-02,  5.5344e-02,\n",
       "                       -1.1993e-01,  9.6906e-02,  1.3825e-02,  3.4695e-02,  1.3838e-01],\n",
       "                      [-4.6040e-02, -3.8617e-02,  1.8678e-01, -1.7257e-01, -1.9192e-01,\n",
       "                        9.5613e-02, -2.0328e-02, -1.7462e-01,  1.7324e-01, -4.9239e-02,\n",
       "                        9.6556e-02, -6.6319e-02, -1.2452e-01, -9.8501e-02, -4.2272e-02,\n",
       "                       -9.9683e-02, -7.3888e-02,  1.1783e-01, -1.7353e-01, -4.7041e-02,\n",
       "                       -1.8023e-02,  5.6839e-02, -6.0361e-02,  1.8998e-01, -1.6731e-01],\n",
       "                      [ 1.7407e-01, -1.5263e-01,  5.8589e-02, -6.2691e-02,  9.1635e-02,\n",
       "                       -1.4444e-01, -1.3851e-01, -4.0016e-02,  1.2459e-01,  9.7340e-02,\n",
       "                        1.9329e-01,  9.4632e-02, -1.9849e-01, -1.4078e-01, -8.5759e-02,\n",
       "                       -7.2495e-03, -1.9182e-01, -1.1670e-01,  1.3349e-01, -9.8308e-02,\n",
       "                        1.4228e-01, -6.2223e-02,  1.3556e-01, -9.2760e-02,  2.3368e-02],\n",
       "                      [-1.8549e-01,  7.3745e-02, -1.0579e-01, -9.8636e-03, -1.3025e-01,\n",
       "                       -5.3015e-03, -1.5499e-01,  1.9673e-01, -1.1843e-01,  1.5340e-01,\n",
       "                       -1.8135e-01, -8.3970e-02, -1.4875e-01, -1.7815e-01, -1.6582e-01,\n",
       "                        1.5915e-01,  9.9506e-02, -8.0955e-04,  1.6761e-01, -8.3580e-02,\n",
       "                       -6.2944e-02, -1.6786e-01, -1.5224e-02, -2.0712e-02, -1.6287e-02],\n",
       "                      [-1.8375e-01,  1.4868e-01,  7.5400e-02, -1.3694e-01,  1.0614e-01,\n",
       "                        1.1729e-01, -7.3075e-02,  4.6951e-02,  1.9498e-01, -1.0723e-01,\n",
       "                       -3.9678e-02,  8.1053e-02,  4.8039e-02, -6.8686e-02, -6.6110e-02,\n",
       "                        1.0983e-01, -6.4138e-02, -1.5413e-01, -1.8414e-01,  7.0699e-02,\n",
       "                        1.6007e-01, -9.7477e-02,  5.4259e-02,  1.7362e-02,  1.3136e-01],\n",
       "                      [-1.6011e-01, -6.9728e-02, -1.8977e-01, -1.1927e-01,  6.6708e-02,\n",
       "                       -1.2695e-01, -9.9703e-02, -1.8753e-01,  1.9588e-01, -6.8191e-02,\n",
       "                       -9.7954e-02,  1.9125e-01,  6.6606e-02,  9.0418e-02,  1.8065e-01,\n",
       "                        8.1986e-02,  9.5527e-02, -1.8757e-01, -2.5081e-02, -2.4478e-02,\n",
       "                       -3.6759e-02,  1.0503e-01, -1.9756e-01,  1.7570e-01,  1.5421e-01],\n",
       "                      [-1.3107e-01, -1.0380e-01,  1.0074e-01, -1.7297e-01,  1.6017e-01,\n",
       "                        1.2846e-01,  1.5297e-01, -7.5437e-02, -1.3506e-01,  6.2060e-02,\n",
       "                        1.1667e-01, -3.4729e-02, -9.7487e-03,  1.6746e-01,  1.2328e-01,\n",
       "                       -8.1025e-03,  1.6323e-01, -1.8796e-01,  1.2938e-02,  2.9850e-03,\n",
       "                        1.9062e-01,  1.5773e-01,  6.0751e-02, -1.9842e-01,  8.0504e-02],\n",
       "                      [ 4.8712e-03,  6.6026e-02,  5.4263e-02, -7.6933e-02, -1.8627e-01,\n",
       "                        2.2807e-02, -1.8950e-02,  1.8554e-01, -1.8683e-01,  9.4436e-02,\n",
       "                       -1.4912e-01,  8.5725e-02,  6.7385e-02, -8.5233e-02,  9.4738e-02,\n",
       "                       -9.1043e-02, -7.0034e-02, -2.8897e-02,  1.5500e-01, -3.1236e-02,\n",
       "                        1.1146e-01, -8.1844e-02, -6.2167e-02,  1.7369e-02,  1.3202e-01],\n",
       "                      [-1.5713e-01,  1.0841e-01,  1.5938e-04, -1.4936e-01,  2.0585e-02,\n",
       "                       -4.2239e-02, -6.2750e-02, -1.6944e-02,  4.7434e-02,  3.7523e-02,\n",
       "                       -1.5048e-01, -4.7634e-02, -1.3533e-01, -1.9238e-01, -1.2099e-02,\n",
       "                        2.0454e-02, -1.6174e-01,  6.5543e-02, -4.2655e-02,  6.4637e-02,\n",
       "                       -1.4966e-01, -7.6602e-02,  7.5893e-04,  6.7065e-02,  4.8893e-02],\n",
       "                      [ 1.8021e-01, -1.0685e-01, -1.9560e-01,  5.7504e-02,  1.9342e-01,\n",
       "                        1.5280e-01, -1.0462e-01, -1.7415e-01,  1.3365e-01, -1.9276e-01,\n",
       "                       -2.0000e-01, -6.9915e-02,  6.5620e-02,  1.6552e-01,  1.1292e-01,\n",
       "                       -1.2710e-01,  7.1986e-02,  1.2909e-02,  1.7057e-01, -6.1837e-02,\n",
       "                       -1.4395e-01, -1.0130e-02,  1.1329e-01,  7.7319e-04, -7.3738e-02],\n",
       "                      [ 1.0661e-01,  1.6127e-02, -9.8570e-03,  1.2344e-01,  1.4355e-01,\n",
       "                       -1.6088e-01,  7.4982e-02,  1.9990e-01,  1.9899e-01,  1.5768e-01,\n",
       "                       -6.4986e-02,  1.5643e-01, -4.9117e-02,  1.2718e-01,  8.0305e-02,\n",
       "                        2.7652e-02, -1.1235e-01, -1.6674e-01,  9.4482e-02,  3.5039e-02,\n",
       "                        1.9218e-02, -1.1396e-01, -9.7452e-02, -9.8433e-02, -9.1849e-02],\n",
       "                      [-8.2199e-02,  1.2830e-01,  8.9203e-02, -2.7928e-02, -1.6123e-02,\n",
       "                        1.4638e-01,  1.7069e-01, -7.7054e-02, -5.2791e-02,  1.2138e-01,\n",
       "                        5.5903e-02,  1.5558e-01, -1.4968e-01,  2.8166e-02, -6.3891e-02,\n",
       "                       -1.1208e-01,  4.0027e-02,  1.1838e-01, -7.3311e-02,  5.0823e-02,\n",
       "                        8.3206e-03,  7.4712e-02, -1.3157e-01,  9.6819e-02,  9.6990e-02],\n",
       "                      [ 5.8762e-02,  1.6969e-01,  1.4618e-01, -4.9180e-02, -1.0662e-01,\n",
       "                       -1.4090e-01,  1.1401e-01,  1.7838e-01, -1.4412e-01, -1.5097e-01,\n",
       "                       -7.7181e-02, -7.0892e-02,  1.6668e-02, -6.3894e-02,  1.8809e-01,\n",
       "                        6.6323e-02, -8.9395e-02, -2.1102e-02,  7.0550e-02, -1.9650e-01,\n",
       "                       -9.5695e-02, -1.1812e-01, -1.4622e-01, -1.3922e-01,  1.2261e-01],\n",
       "                      [ 1.3765e-01,  1.6623e-01,  8.3289e-02,  1.1260e-01, -9.7510e-02,\n",
       "                        1.0612e-02,  7.9891e-02,  3.7010e-02, -1.9253e-01,  1.4881e-01,\n",
       "                       -1.4242e-01, -2.9444e-02, -8.6093e-03,  3.3446e-02,  1.9092e-01,\n",
       "                       -1.0713e-01, -9.1837e-02, -1.3774e-01, -1.8336e-01,  2.2863e-02,\n",
       "                       -7.9795e-02, -3.9368e-02, -1.1603e-01,  8.9204e-02,  9.3406e-02],\n",
       "                      [ 1.2939e-01,  1.5374e-01,  9.8185e-03,  2.5947e-03,  1.3963e-01,\n",
       "                        1.7837e-01, -1.9507e-01, -1.1190e-01, -5.4499e-02,  1.8952e-02,\n",
       "                       -1.2941e-01, -6.0213e-02, -1.1807e-01, -1.5524e-01,  7.9946e-02,\n",
       "                        1.6078e-01,  1.1912e-01,  1.9450e-01,  1.2564e-01, -1.9408e-01,\n",
       "                        1.4160e-01, -1.9681e-01, -4.6049e-02, -1.7012e-02,  1.5254e-01],\n",
       "                      [ 1.5877e-01,  2.0329e-02,  3.6830e-02, -1.0857e-01, -6.7924e-02,\n",
       "                       -1.7521e-01, -1.3534e-01, -1.1939e-02,  1.6076e-01, -1.0752e-01,\n",
       "                       -1.4720e-01, -1.2960e-01,  5.1141e-02,  8.3628e-02,  8.2340e-02,\n",
       "                       -5.3574e-02,  1.3853e-03,  1.7316e-01, -5.7262e-02,  4.5111e-02,\n",
       "                        1.5488e-01, -1.6032e-01, -1.0539e-01,  2.5407e-02, -1.2028e-01],\n",
       "                      [-1.1187e-01, -1.0698e-01, -1.1344e-01, -1.6420e-01,  1.2354e-02,\n",
       "                       -1.2966e-01,  2.0961e-02, -1.8141e-01, -9.1850e-02,  1.9365e-01,\n",
       "                        1.4126e-01, -1.1762e-01,  2.4678e-02, -1.7831e-01,  1.3878e-01,\n",
       "                        1.9369e-01, -1.3785e-01, -1.9023e-01,  1.1433e-02, -1.3839e-01,\n",
       "                       -6.3757e-02, -1.5715e-01,  5.7608e-02, -6.8087e-02, -1.1610e-01],\n",
       "                      [-1.4919e-01, -1.1925e-01,  6.7081e-02,  1.8187e-01, -8.2983e-02,\n",
       "                        1.2716e-02, -1.9624e-02, -3.8356e-02, -1.0387e-01, -8.9531e-02,\n",
       "                        1.9139e-01, -1.7301e-01, -8.5278e-03,  7.8753e-02,  1.8635e-01,\n",
       "                       -1.4525e-01, -1.5906e-01,  3.6669e-02, -3.3636e-03,  1.2299e-01,\n",
       "                       -7.6643e-02, -5.3120e-02, -9.9873e-02,  5.8829e-02, -6.4343e-02]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layer.self_attn.out_proj.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0.], device='cuda:0')),\n",
       "             ('encoder_layer.linear1.weight',\n",
       "              tensor([[ 0.1525, -0.0639, -0.0040,  ..., -0.0443,  0.1246, -0.1300],\n",
       "                      [-0.0094,  0.1922, -0.1031,  ..., -0.0464,  0.1191,  0.0526],\n",
       "                      [-0.0333,  0.1076, -0.1221,  ..., -0.0014,  0.1383,  0.1313],\n",
       "                      ...,\n",
       "                      [ 0.1003, -0.0246, -0.1632,  ..., -0.0541,  0.0758,  0.1181],\n",
       "                      [-0.1692,  0.1402,  0.1120,  ..., -0.1344, -0.1518, -0.0016],\n",
       "                      [-0.0724, -0.1647,  0.0020,  ...,  0.1973, -0.1285, -0.1780]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layer.linear1.bias',\n",
       "              tensor([-0.1616, -0.1964,  0.1164,  ...,  0.1288, -0.1367,  0.0015],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layer.linear2.weight',\n",
       "              tensor([[-0.0047, -0.0045, -0.0003,  ..., -0.0101,  0.0004,  0.0219],\n",
       "                      [-0.0167,  0.0187,  0.0192,  ..., -0.0199, -0.0033,  0.0100],\n",
       "                      [-0.0152,  0.0095, -0.0080,  ...,  0.0091,  0.0053, -0.0021],\n",
       "                      ...,\n",
       "                      [ 0.0031, -0.0074,  0.0033,  ...,  0.0150,  0.0134, -0.0010],\n",
       "                      [ 0.0056,  0.0195, -0.0032,  ...,  0.0151, -0.0134,  0.0026],\n",
       "                      [ 0.0210, -0.0067, -0.0059,  ...,  0.0008,  0.0088,  0.0184]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder_layer.linear2.bias',\n",
       "              tensor([-0.0075,  0.0137,  0.0067, -0.0111,  0.0093,  0.0011,  0.0165,  0.0218,\n",
       "                       0.0132,  0.0012,  0.0020,  0.0026,  0.0117,  0.0087, -0.0130,  0.0047,\n",
       "                       0.0210,  0.0066,  0.0181, -0.0129,  0.0030,  0.0059,  0.0110, -0.0051,\n",
       "                      -0.0126], device='cuda:0')),\n",
       "             ('encoder_layer.norm1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n",
       "             ('encoder_layer.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0.], device='cuda:0')),\n",
       "             ('encoder_layer.norm2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n",
       "             ('encoder_layer.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0.], device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.self_attn.in_proj_weight',\n",
       "              tensor([[-0.1058, -0.1755,  0.0660,  ...,  0.0411, -0.0683, -0.0715],\n",
       "                      [-0.2097, -0.0324, -0.1007,  ..., -0.2534, -0.0059, -0.0795],\n",
       "                      [ 0.2506, -0.1225, -0.1167,  ..., -0.2026, -0.0181, -0.0135],\n",
       "                      ...,\n",
       "                      [ 0.0306, -0.0565,  0.0645,  ...,  0.1414, -0.0554, -0.2014],\n",
       "                      [ 0.0865, -0.1703,  0.0155,  ..., -0.1364,  0.1351, -0.0241],\n",
       "                      [ 0.1873, -0.0035,  0.0546,  ...,  0.0548, -0.0022,  0.0852]],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.self_attn.in_proj_bias',\n",
       "              tensor([-3.8334e-03, -8.6280e-04, -2.6579e-03, -6.9411e-03, -2.5735e-03,\n",
       "                       1.3328e-02, -1.3793e-02,  5.7300e-03, -1.0283e-02, -1.2981e-02,\n",
       "                      -4.1681e-03, -8.0278e-03,  1.9266e-03,  5.3896e-03,  6.3753e-03,\n",
       "                       1.8255e-02,  1.4809e-02,  7.7211e-03, -1.3178e-02, -1.1475e-02,\n",
       "                      -2.1914e-03, -7.2393e-03, -1.8016e-03, -4.0650e-03,  2.6793e-04,\n",
       "                      -7.7172e-05, -3.7042e-05,  5.1538e-05, -2.3822e-05,  2.6776e-06,\n",
       "                       1.8876e-05, -3.0175e-05, -2.0530e-05,  6.6136e-05,  6.0127e-06,\n",
       "                      -1.4130e-05,  7.8393e-05,  7.1411e-05,  3.5214e-05, -4.6458e-05,\n",
       "                      -6.7866e-05, -7.0320e-05,  6.8444e-06, -1.5557e-05,  6.1682e-05,\n",
       "                       5.8912e-05, -1.5591e-05, -1.2179e-05, -1.6940e-05,  1.0750e-05,\n",
       "                      -5.1837e-03, -5.2174e-03, -1.7773e-02, -2.0176e-02, -2.0675e-02,\n",
       "                       1.2181e-03,  5.4686e-03, -2.9568e-02, -2.1210e-02, -1.8921e-03,\n",
       "                      -2.0197e-03, -6.9533e-03, -1.0478e-02, -8.5184e-03,  3.3050e-02,\n",
       "                      -2.2803e-02,  1.8795e-02,  2.8987e-02,  1.4302e-02, -3.0392e-04,\n",
       "                      -9.2982e-03,  1.9624e-02, -2.2958e-02,  2.1484e-02,  4.2721e-03],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.0752, -0.0843, -0.0678, -0.0751, -0.0863, -0.1843,  0.1867,  0.1665,\n",
       "                        0.0994, -0.1528,  0.0792, -0.1391,  0.1439,  0.1273, -0.1468,  0.1356,\n",
       "                        0.0977, -0.1320, -0.1017, -0.1041, -0.1576, -0.1323,  0.0689,  0.0220,\n",
       "                       -0.0257],\n",
       "                      [-0.1541,  0.0165, -0.1514, -0.1687,  0.0038, -0.1935, -0.0316, -0.1624,\n",
       "                       -0.1292, -0.0824, -0.1057,  0.0840, -0.1192,  0.0048, -0.0075, -0.0978,\n",
       "                        0.0988,  0.1500, -0.0870,  0.0399,  0.1239, -0.0321, -0.2011, -0.0206,\n",
       "                       -0.0131],\n",
       "                      [-0.0869, -0.0520,  0.1118, -0.1953,  0.1266, -0.1638,  0.1427,  0.0626,\n",
       "                        0.0505, -0.0891, -0.1427,  0.1289, -0.1371,  0.0891, -0.1807,  0.1134,\n",
       "                        0.0135, -0.1465,  0.2083,  0.0642,  0.0500,  0.0890, -0.0860, -0.1626,\n",
       "                       -0.0823],\n",
       "                      [-0.0380,  0.1228,  0.0411, -0.0379, -0.1265,  0.1240, -0.1403,  0.0717,\n",
       "                        0.0086,  0.1835,  0.1867, -0.0101,  0.0502,  0.0880,  0.1792, -0.0078,\n",
       "                       -0.1301,  0.0218,  0.0833,  0.0316, -0.0336,  0.0033, -0.0206, -0.1714,\n",
       "                       -0.1144],\n",
       "                      [ 0.1296, -0.0179, -0.0050, -0.1916,  0.1495, -0.0027, -0.1095, -0.1249,\n",
       "                        0.2048,  0.1270,  0.1283,  0.0690,  0.1084,  0.1358,  0.0743,  0.1080,\n",
       "                       -0.1373, -0.1803, -0.1775, -0.1851,  0.0132,  0.2087, -0.0833, -0.1205,\n",
       "                        0.1484],\n",
       "                      [ 0.0265,  0.0246,  0.1781,  0.0763, -0.1747, -0.1065,  0.0190,  0.0826,\n",
       "                        0.1425,  0.1569, -0.1239, -0.0973, -0.1082, -0.1026, -0.1291,  0.0318,\n",
       "                       -0.0288, -0.0135, -0.0974, -0.1592,  0.0986, -0.0263,  0.0267, -0.1858,\n",
       "                       -0.1390],\n",
       "                      [ 0.1869, -0.0619,  0.0827,  0.1440,  0.1173, -0.1200,  0.1904,  0.1499,\n",
       "                        0.0382,  0.0840, -0.1227,  0.1715,  0.1097,  0.1043, -0.0283, -0.1012,\n",
       "                       -0.0227, -0.1818, -0.1186, -0.1627,  0.1002, -0.0317,  0.0702,  0.0469,\n",
       "                       -0.0372],\n",
       "                      [-0.1928,  0.1842, -0.1458, -0.1842, -0.1556,  0.0925,  0.1496, -0.0499,\n",
       "                        0.1076, -0.1669,  0.2073,  0.0935,  0.0949,  0.0044,  0.1114, -0.0741,\n",
       "                        0.1596, -0.1501, -0.0444,  0.0425, -0.1044,  0.0869, -0.0103,  0.0194,\n",
       "                        0.1190],\n",
       "                      [-0.0558, -0.0260,  0.1772, -0.1571, -0.1757,  0.0843, -0.0349, -0.1586,\n",
       "                        0.1750, -0.0340,  0.0779, -0.0427, -0.1116, -0.0948, -0.0574, -0.0828,\n",
       "                       -0.0886,  0.1138, -0.1879, -0.0594, -0.0211,  0.0332, -0.0457,  0.1801,\n",
       "                       -0.1929],\n",
       "                      [ 0.1890, -0.1659,  0.0677, -0.0764,  0.0778, -0.1043, -0.1211, -0.0499,\n",
       "                        0.1035,  0.0921,  0.1818,  0.0853, -0.2087, -0.1581, -0.0724, -0.0211,\n",
       "                       -0.1736, -0.0933,  0.1438, -0.1340,  0.1239, -0.0493,  0.1495, -0.0743,\n",
       "                        0.0344],\n",
       "                      [-0.1534,  0.0519, -0.0694, -0.0283, -0.1499,  0.0274, -0.1286,  0.1819,\n",
       "                       -0.1213,  0.1141, -0.2101, -0.0966, -0.1702, -0.2080, -0.1457,  0.1394,\n",
       "                        0.1227,  0.0358,  0.1719, -0.1437, -0.0774, -0.1476,  0.0264, -0.0021,\n",
       "                        0.0016],\n",
       "                      [-0.1523,  0.1146,  0.1075, -0.1671,  0.0709,  0.1241, -0.0400,  0.0247,\n",
       "                        0.1730, -0.1274, -0.0376,  0.0445,  0.0197, -0.0865, -0.0366,  0.0802,\n",
       "                       -0.0336, -0.1290, -0.1505,  0.0552,  0.1446, -0.0655,  0.0727,  0.0400,\n",
       "                        0.1593],\n",
       "                      [-0.1782, -0.0551, -0.1971, -0.1043,  0.0856, -0.1302, -0.1158, -0.1751,\n",
       "                        0.2050, -0.0668, -0.0734,  0.1987,  0.0769,  0.1117,  0.1663,  0.0997,\n",
       "                        0.0778, -0.2055, -0.0339, -0.0008, -0.0271,  0.0881, -0.1916,  0.1616,\n",
       "                        0.1365],\n",
       "                      [-0.1410, -0.0986,  0.0899, -0.1709,  0.1629,  0.1118,  0.1497, -0.0731,\n",
       "                       -0.1264,  0.0856,  0.1343, -0.0368, -0.0054,  0.1725,  0.1221, -0.0054,\n",
       "                        0.1636, -0.1968,  0.0186,  0.0300,  0.1786,  0.1447,  0.0718, -0.1964,\n",
       "                        0.0682],\n",
       "                      [-0.0196,  0.0864,  0.0167, -0.0594, -0.1657,  0.0039, -0.0428,  0.1954,\n",
       "                       -0.1769,  0.1020, -0.1389,  0.1032,  0.0821, -0.0714,  0.0767, -0.0712,\n",
       "                       -0.0933, -0.0543,  0.1379, -0.0006,  0.1037, -0.1055, -0.0824,  0.0040,\n",
       "                        0.1050],\n",
       "                      [-0.1599,  0.1095,  0.0015, -0.1520,  0.0146, -0.0439, -0.0582, -0.0164,\n",
       "                        0.0452,  0.0339, -0.1260, -0.0578, -0.1307, -0.1817, -0.0115,  0.0177,\n",
       "                       -0.1571,  0.0696, -0.0422,  0.0616, -0.1505, -0.0736, -0.0072,  0.0694,\n",
       "                        0.0537],\n",
       "                      [ 0.1760, -0.1005, -0.2018,  0.0599,  0.1964,  0.1616, -0.1053, -0.1734,\n",
       "                        0.1260, -0.1750, -0.2020, -0.0631,  0.0685,  0.1631,  0.1107, -0.1216,\n",
       "                        0.0683,  0.0127,  0.1648, -0.0701, -0.1438, -0.0130,  0.1028, -0.0010,\n",
       "                       -0.0781],\n",
       "                      [ 0.1130,  0.0044,  0.0140,  0.1083,  0.1257, -0.1465,  0.0868,  0.1883,\n",
       "                        0.2065,  0.1386, -0.0729,  0.1460, -0.0671,  0.1122,  0.0922,  0.0158,\n",
       "                       -0.0982, -0.1564,  0.1049,  0.0414,  0.0156, -0.0997, -0.0916, -0.0903,\n",
       "                       -0.0787],\n",
       "                      [-0.0798,  0.1259,  0.0868, -0.0268, -0.0159,  0.1453,  0.1706, -0.0800,\n",
       "                       -0.0576,  0.0955,  0.0315,  0.1650, -0.1492,  0.0196, -0.0665, -0.1125,\n",
       "                        0.0388,  0.1188, -0.0762,  0.0441, -0.0049,  0.0708, -0.1245,  0.0989,\n",
       "                        0.0916],\n",
       "                      [ 0.0659,  0.1706,  0.1405, -0.0467, -0.1036, -0.1214,  0.1147,  0.1745,\n",
       "                       -0.1610, -0.1502, -0.1165, -0.0541,  0.0103, -0.0857,  0.1881,  0.0642,\n",
       "                       -0.0896, -0.0057,  0.0618, -0.2398, -0.0915, -0.1071, -0.1591, -0.1394,\n",
       "                        0.1255],\n",
       "                      [ 0.1323,  0.1681,  0.0776,  0.1101, -0.0946,  0.0142,  0.0780,  0.0355,\n",
       "                       -0.1888,  0.1677, -0.1326, -0.0335, -0.0102,  0.0332,  0.1941, -0.1043,\n",
       "                       -0.0923, -0.1424, -0.1774,  0.0295, -0.0756, -0.0384, -0.1208,  0.0885,\n",
       "                        0.0940],\n",
       "                      [ 0.1243,  0.1592, -0.0048,  0.0153,  0.1518,  0.1737, -0.2034, -0.1025,\n",
       "                       -0.0630,  0.0088, -0.1432, -0.0409, -0.1104, -0.1522,  0.0697,  0.1688,\n",
       "                        0.1061,  0.1953,  0.1073, -0.2187,  0.1503, -0.2065, -0.0550, -0.0303,\n",
       "                        0.1347],\n",
       "                      [ 0.1503,  0.0294,  0.0287, -0.1041, -0.0613, -0.1632, -0.1378, -0.0093,\n",
       "                        0.1525, -0.0903, -0.1367, -0.1231,  0.0569,  0.0852,  0.0777, -0.0465,\n",
       "                       -0.0044,  0.1622, -0.0550,  0.0645,  0.1569, -0.1617, -0.1206,  0.0210,\n",
       "                       -0.1240],\n",
       "                      [-0.1265, -0.0955, -0.1247, -0.1515,  0.0194, -0.1420,  0.0095, -0.1653,\n",
       "                       -0.0714,  0.1757,  0.1117, -0.0955,  0.0317, -0.1809,  0.1235,  0.2039,\n",
       "                       -0.1497, -0.2070,  0.0009, -0.1178, -0.0478, -0.1616,  0.0480, -0.0833,\n",
       "                       -0.1188],\n",
       "                      [-0.1483, -0.1236,  0.0635,  0.1734, -0.0944,  0.0031, -0.0148, -0.0399,\n",
       "                       -0.0978, -0.0845,  0.1782, -0.1754, -0.0115,  0.0735,  0.1898, -0.1490,\n",
       "                       -0.1517,  0.0415,  0.0025,  0.1179, -0.0773, -0.0464, -0.1008,  0.0604,\n",
       "                       -0.0544]], device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.self_attn.out_proj.bias',\n",
       "              tensor([-0.0093,  0.0264, -0.0090, -0.0037, -0.0078,  0.0013, -0.0103,  0.0153,\n",
       "                       0.0126, -0.0167, -0.0237, -0.0309,  0.0153,  0.0025,  0.0217, -0.0005,\n",
       "                       0.0022, -0.0125,  0.0013, -0.0006, -0.0013,  0.0109,  0.0043,  0.0135,\n",
       "                      -0.0046], device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.linear1.weight',\n",
       "              tensor([[ 0.1800, -0.0568,  0.0011,  ..., -0.0877,  0.1302, -0.1659],\n",
       "                      [-0.0341,  0.1962, -0.1047,  ..., -0.0553,  0.1454,  0.0576],\n",
       "                      [-0.0885,  0.1104, -0.0773,  ...,  0.0230,  0.1749,  0.1745],\n",
       "                      ...,\n",
       "                      [ 0.1083, -0.0236, -0.1715,  ..., -0.0816,  0.0858,  0.1049],\n",
       "                      [-0.1522,  0.1679,  0.1010,  ..., -0.1410, -0.1632, -0.0301],\n",
       "                      [-0.0891, -0.1694,  0.0086,  ...,  0.1781, -0.0824, -0.1676]],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.linear1.bias',\n",
       "              tensor([-0.1709, -0.2006,  0.1218,  ...,  0.1011, -0.1554,  0.0006],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.linear2.weight',\n",
       "              tensor([[ 1.2203e-02,  3.2230e-03, -2.3458e-02,  ...,  9.0188e-03,\n",
       "                       -1.8330e-02, -4.9260e-04],\n",
       "                      [-4.7163e-02,  1.6680e-02,  3.3656e-02,  ..., -2.9648e-02,\n",
       "                       -2.1459e-02,  1.4945e-02],\n",
       "                      [-2.5788e-02,  3.8680e-03,  9.3554e-03,  ..., -6.6867e-04,\n",
       "                        1.5566e-02,  2.0464e-02],\n",
       "                      ...,\n",
       "                      [ 9.7251e-03, -1.0037e-02, -2.7918e-03,  ..., -4.3171e-03,\n",
       "                        3.3076e-05,  2.7071e-03],\n",
       "                      [ 7.6089e-03,  2.8917e-02,  1.6832e-02,  ...,  2.0428e-02,\n",
       "                       -2.0493e-03, -2.4253e-03],\n",
       "                      [ 2.7414e-02, -8.4575e-03, -8.2520e-03,  ...,  2.7419e-03,\n",
       "                        2.6582e-02,  1.7103e-02]], device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.linear2.bias',\n",
       "              tensor([-0.0094,  0.0084,  0.0097, -0.0076,  0.0143,  0.0033,  0.0191,  0.0189,\n",
       "                       0.0177,  0.0026,  0.0024,  0.0010,  0.0125,  0.0002, -0.0133,  0.0087,\n",
       "                       0.0182,  0.0063,  0.0147, -0.0094,  0.0070,  0.0021,  0.0131, -0.0032,\n",
       "                      -0.0071], device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.norm1.weight',\n",
       "              tensor([1.1304, 0.9982, 1.0464, 0.9473, 1.0119, 0.9630, 1.0399, 0.9959, 0.9806,\n",
       "                      1.0031, 0.9701, 1.0727, 0.9873, 0.9588, 1.0019, 0.9477, 0.9911, 1.0396,\n",
       "                      0.9756, 0.9604, 1.0234, 0.9643, 1.0311, 1.1230, 1.0068],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.norm1.bias',\n",
       "              tensor([ 0.0025,  0.0307, -0.0036,  0.0260, -0.0125,  0.0242,  0.0100,  0.0115,\n",
       "                       0.0041, -0.0322, -0.0007, -0.0012,  0.0179, -0.0117,  0.0300, -0.0181,\n",
       "                      -0.0295,  0.0136,  0.0182, -0.0129, -0.0092,  0.0129, -0.0257,  0.0156,\n",
       "                      -0.0191], device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.norm2.weight',\n",
       "              tensor([1.0623, 0.9742, 0.9959, 1.0224, 0.9782, 1.0006, 1.0160, 0.9997, 0.9893,\n",
       "                      0.9854, 1.0139, 1.0096, 0.9623, 0.9627, 0.9852, 0.9996, 0.9942, 0.9560,\n",
       "                      0.9650, 1.0020, 1.0043, 0.9850, 1.0327, 1.0571, 0.9856],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.0.norm2.bias',\n",
       "              tensor([ 0.0050,  0.0023, -0.0150,  0.0017, -0.0120,  0.0194,  0.0220,  0.0013,\n",
       "                       0.0030, -0.0307,  0.0238,  0.0138,  0.0159, -0.0065,  0.0249, -0.0119,\n",
       "                      -0.0233,  0.0146,  0.0059, -0.0267, -0.0113,  0.0091, -0.0044, -0.0118,\n",
       "                      -0.0186], device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.self_attn.in_proj_weight',\n",
       "              tensor([[-0.1261, -0.1584,  0.0607,  ...,  0.0423, -0.0231, -0.0924],\n",
       "                      [-0.1918, -0.0754, -0.0956,  ..., -0.2512, -0.0539, -0.0431],\n",
       "                      [ 0.2241, -0.1249, -0.0959,  ..., -0.2210,  0.0235,  0.0027],\n",
       "                      ...,\n",
       "                      [ 0.0128, -0.0527,  0.0352,  ...,  0.1579, -0.0829, -0.1852],\n",
       "                      [ 0.0941, -0.1356,  0.0414,  ..., -0.1631,  0.1551, -0.0396],\n",
       "                      [ 0.1921, -0.0073,  0.0596,  ...,  0.0685, -0.0089,  0.0768]],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.self_attn.in_proj_bias',\n",
       "              tensor([-1.0281e-02,  4.1186e-03,  1.9829e-03, -7.2530e-03,  6.0510e-03,\n",
       "                      -9.9186e-04,  3.3306e-03, -5.6779e-04,  3.2942e-03, -7.7341e-03,\n",
       "                      -3.6320e-04, -6.6823e-03,  8.1194e-03,  5.5608e-03,  5.6293e-03,\n",
       "                      -1.8329e-02,  2.1729e-02, -4.8452e-03,  1.4293e-02, -1.8010e-02,\n",
       "                       6.5428e-04,  1.9453e-02,  5.7389e-03, -5.5948e-03,  6.8129e-03,\n",
       "                       2.4588e-05, -3.0810e-05,  5.4439e-05,  1.3202e-05, -6.0094e-05,\n",
       "                       1.5594e-05,  1.7722e-05,  6.9699e-05, -2.6444e-06,  4.8661e-05,\n",
       "                       2.3550e-06, -1.8774e-05,  3.1477e-05,  3.0159e-05, -1.0592e-06,\n",
       "                       4.6574e-05,  3.9439e-05,  9.1636e-06,  2.5848e-06, -5.7414e-06,\n",
       "                       1.2513e-06, -1.2821e-05,  3.4901e-05, -1.5619e-05,  2.9853e-05,\n",
       "                       2.9737e-03, -6.5180e-03,  1.3371e-02, -7.5488e-03, -9.7857e-03,\n",
       "                      -2.6264e-03, -1.0173e-02, -1.1600e-02, -9.4333e-03,  1.0116e-02,\n",
       "                      -8.2023e-04, -1.0823e-02, -6.9776e-03, -8.9550e-03,  1.6364e-02,\n",
       "                      -6.5557e-03,  1.9088e-03,  2.3226e-02,  8.6198e-03, -1.7350e-02,\n",
       "                      -3.4648e-03,  8.5735e-03, -1.3308e-02,  3.7410e-03, -5.6026e-03],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.self_attn.out_proj.weight',\n",
       "              tensor([[ 0.0394, -0.0722, -0.0727, -0.0794, -0.0896, -0.2148,  0.1545,  0.1550,\n",
       "                        0.1744, -0.1174,  0.1174, -0.1415,  0.1573,  0.1508, -0.1508,  0.1208,\n",
       "                        0.0939, -0.1671, -0.0959, -0.0653, -0.1508, -0.1498,  0.0398,  0.0152,\n",
       "                       -0.0277],\n",
       "                      [-0.1027, -0.0043, -0.1272, -0.1822, -0.0336, -0.1466, -0.0172, -0.1738,\n",
       "                       -0.1281, -0.0657, -0.1239,  0.0732, -0.1377, -0.0256,  0.0181, -0.1016,\n",
       "                        0.1265,  0.1845, -0.0724, -0.0093,  0.1169, -0.0185, -0.1888,  0.0060,\n",
       "                        0.0131],\n",
       "                      [-0.0789, -0.0598,  0.0949, -0.1721,  0.1354, -0.1678,  0.1504,  0.0641,\n",
       "                        0.0199, -0.0688, -0.1274,  0.1183, -0.1400,  0.0845, -0.1727,  0.1163,\n",
       "                        0.0184, -0.1335,  0.2143,  0.0385,  0.0510,  0.0840, -0.0859, -0.1681,\n",
       "                       -0.0817],\n",
       "                      [-0.0375,  0.1333,  0.0386, -0.0294, -0.1017,  0.1541, -0.1248,  0.0669,\n",
       "                       -0.0208,  0.1453,  0.1446,  0.0071,  0.0554,  0.0710,  0.1661, -0.0177,\n",
       "                       -0.1383,  0.0197,  0.0570, -0.0144, -0.0236, -0.0129, -0.0130, -0.1890,\n",
       "                       -0.1147],\n",
       "                      [ 0.1044, -0.0112, -0.0215, -0.1803,  0.1588, -0.0234, -0.0854, -0.1216,\n",
       "                        0.1900,  0.1274,  0.1367,  0.0718,  0.1116,  0.1508,  0.0693,  0.1166,\n",
       "                       -0.1427, -0.1937, -0.1852, -0.1988,  0.0195,  0.2052, -0.0978, -0.1326,\n",
       "                        0.1525],\n",
       "                      [ 0.0401,  0.0311,  0.1732,  0.0698, -0.1793, -0.1004, -0.0012,  0.0750,\n",
       "                        0.1717,  0.1531, -0.1529, -0.0815, -0.1101, -0.1014, -0.1340,  0.0194,\n",
       "                       -0.0197, -0.0230, -0.1009, -0.1644,  0.1123, -0.0399,  0.0108, -0.1904,\n",
       "                       -0.1320],\n",
       "                      [ 0.1763, -0.0605,  0.0721,  0.1509,  0.1259, -0.1460,  0.1878,  0.1647,\n",
       "                        0.0264,  0.1009, -0.1215,  0.1766,  0.1300,  0.0942, -0.0314, -0.0970,\n",
       "                       -0.0178, -0.1836, -0.1125, -0.1659,  0.0873, -0.0342,  0.0554,  0.0438,\n",
       "                       -0.0505],\n",
       "                      [-0.1844,  0.1752, -0.1167, -0.1941, -0.1645,  0.1011,  0.1448, -0.0523,\n",
       "                        0.0850, -0.1834,  0.1810,  0.0860,  0.0615, -0.0145,  0.1214, -0.0899,\n",
       "                        0.1712, -0.1506, -0.0435,  0.0634, -0.1223,  0.1048,  0.0229,  0.0340,\n",
       "                        0.1363],\n",
       "                      [-0.0597, -0.0222,  0.1791, -0.1724, -0.1956,  0.0840, -0.0261, -0.1674,\n",
       "                        0.1858, -0.0248,  0.0882, -0.0521, -0.1239, -0.0833, -0.0507, -0.0856,\n",
       "                       -0.0786,  0.0968, -0.1642, -0.0348, -0.0096,  0.0237, -0.0687,  0.1879,\n",
       "                       -0.1741],\n",
       "                      [ 0.1791, -0.1596,  0.0640, -0.0664,  0.0834, -0.1398, -0.1096, -0.0585,\n",
       "                        0.1197,  0.1208,  0.2045,  0.0843, -0.1977, -0.1496, -0.0727, -0.0129,\n",
       "                       -0.1901, -0.1071,  0.1392, -0.1091,  0.1302, -0.0679,  0.1287, -0.0837,\n",
       "                        0.0481],\n",
       "                      [-0.1717,  0.0723, -0.0918, -0.0170, -0.1243, -0.0050, -0.1641,  0.1963,\n",
       "                       -0.1025,  0.1207, -0.1634, -0.0819, -0.1416, -0.1654, -0.1687,  0.1673,\n",
       "                        0.0950,  0.0168,  0.1638, -0.0925, -0.0604, -0.1520, -0.0147, -0.0320,\n",
       "                       -0.0113],\n",
       "                      [-0.1748,  0.1311,  0.0954, -0.1532,  0.0841,  0.1152, -0.0582,  0.0405,\n",
       "                        0.1862, -0.1017, -0.0280,  0.0589,  0.0503, -0.0723, -0.0472,  0.0831,\n",
       "                       -0.0355, -0.1365, -0.1614,  0.0501,  0.1364, -0.0699,  0.0531,  0.0225,\n",
       "                        0.1528],\n",
       "                      [-0.1661, -0.0644, -0.1994, -0.1096,  0.0805, -0.1205, -0.1203, -0.1732,\n",
       "                        0.1743, -0.0749, -0.1103,  0.2028,  0.0733,  0.0895,  0.1727,  0.1047,\n",
       "                        0.0977, -0.1881, -0.0296, -0.0131, -0.0249,  0.1084, -0.1926,  0.1554,\n",
       "                        0.1334],\n",
       "                      [-0.1203, -0.0951,  0.0907, -0.1589,  0.1491,  0.1218,  0.1301, -0.0863,\n",
       "                       -0.0952,  0.0455,  0.1084, -0.0242, -0.0367,  0.1525,  0.1166, -0.0053,\n",
       "                        0.1458, -0.2020, -0.0025,  0.0139,  0.1870,  0.1387,  0.0685, -0.1817,\n",
       "                        0.0713],\n",
       "                      [ 0.0007,  0.0844,  0.0577, -0.0750, -0.1683,  0.0065, -0.0439,  0.1944,\n",
       "                       -0.1614,  0.0740, -0.1490,  0.0987,  0.0774, -0.0589,  0.0739, -0.0836,\n",
       "                       -0.0882, -0.0420,  0.1307, -0.0147,  0.1285, -0.0908, -0.0544,  0.0071,\n",
       "                        0.1104],\n",
       "                      [-0.1412,  0.1012,  0.0008, -0.1558,  0.0314, -0.0589, -0.0476, -0.0107,\n",
       "                        0.0377,  0.0485, -0.1242, -0.0617, -0.1134, -0.1774, -0.0061,  0.0199,\n",
       "                       -0.1450,  0.0703, -0.0333,  0.0557, -0.1418, -0.0713,  0.0094,  0.0556,\n",
       "                        0.0593],\n",
       "                      [ 0.1745, -0.1140, -0.1859,  0.0501,  0.1748,  0.1503, -0.0842, -0.1785,\n",
       "                        0.1120, -0.1591, -0.1893, -0.0833,  0.0610,  0.1595,  0.1276, -0.1391,\n",
       "                        0.0740,  0.0247,  0.1785, -0.0742, -0.1529, -0.0140,  0.1020,  0.0115,\n",
       "                       -0.0490],\n",
       "                      [ 0.0953,  0.0168,  0.0079,  0.1109,  0.1375, -0.1606,  0.0765,  0.1930,\n",
       "                        0.1985,  0.1401, -0.0464,  0.1531, -0.0305,  0.1261,  0.0808,  0.0334,\n",
       "                       -0.1096, -0.1741,  0.0944,  0.0261,  0.0250, -0.1072, -0.0987, -0.1092,\n",
       "                       -0.0839],\n",
       "                      [-0.0802,  0.1332,  0.0867, -0.0158, -0.0117,  0.1539,  0.1467, -0.0730,\n",
       "                       -0.0704,  0.1075,  0.0535,  0.1612, -0.1482,  0.0392, -0.0734, -0.0954,\n",
       "                        0.0286,  0.1187, -0.0780,  0.0447,  0.0124,  0.0711, -0.1058,  0.0904,\n",
       "                        0.0716],\n",
       "                      [ 0.0365,  0.1708,  0.1265, -0.0367, -0.1141, -0.1425,  0.1118,  0.1691,\n",
       "                       -0.1154, -0.1314, -0.0944, -0.0643, -0.0051, -0.0845,  0.1856,  0.0501,\n",
       "                       -0.1070, -0.0269,  0.0611, -0.1821, -0.0970, -0.1122, -0.1548, -0.1132,\n",
       "                        0.1103],\n",
       "                      [ 0.1452,  0.1545,  0.0829,  0.1079, -0.0947,  0.0093,  0.0963,  0.0300,\n",
       "                       -0.1807,  0.1704, -0.1264, -0.0402,  0.0036,  0.0266,  0.2034, -0.1024,\n",
       "                       -0.0855, -0.1152, -0.1771,  0.0139, -0.0884, -0.0377, -0.1092,  0.0935,\n",
       "                        0.1037],\n",
       "                      [ 0.1251,  0.1642,  0.0097,  0.0073,  0.1457,  0.1767, -0.1966, -0.0997,\n",
       "                       -0.0580, -0.0032, -0.1416, -0.0518, -0.1168, -0.1398,  0.0629,  0.1662,\n",
       "                        0.1075,  0.1841,  0.1053, -0.1844,  0.1510, -0.1939, -0.0353, -0.0236,\n",
       "                        0.1277],\n",
       "                      [ 0.1604,  0.0378,  0.0474, -0.1166, -0.0945, -0.1846, -0.1659, -0.0051,\n",
       "                        0.1957, -0.0707, -0.1428, -0.1158,  0.0506,  0.1011,  0.0756, -0.0347,\n",
       "                       -0.0068,  0.1510, -0.0437,  0.0682,  0.1516, -0.1899, -0.1225,  0.0485,\n",
       "                       -0.1340],\n",
       "                      [-0.1246, -0.1071, -0.1435, -0.1398,  0.0240, -0.1009,  0.0589, -0.1652,\n",
       "                       -0.1477,  0.1522,  0.1095, -0.1073,  0.0131, -0.1865,  0.1271,  0.1830,\n",
       "                       -0.1249, -0.1817,  0.0091, -0.1454, -0.0568, -0.1455,  0.0489, -0.0730,\n",
       "                       -0.1147],\n",
       "                      [-0.1421, -0.1282,  0.0655,  0.1797, -0.0797,  0.0262, -0.0006, -0.0349,\n",
       "                       -0.1232, -0.0891,  0.1928, -0.1843,  0.0063,  0.0745,  0.1924, -0.1466,\n",
       "                       -0.1485,  0.0550,  0.0093,  0.1048, -0.0676, -0.0334, -0.1071,  0.0462,\n",
       "                       -0.0504]], device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.self_attn.out_proj.bias',\n",
       "              tensor([-3.7120e-03,  2.5922e-05, -6.3663e-03,  2.2219e-03,  4.7414e-03,\n",
       "                       4.5426e-03, -9.6207e-03, -5.6151e-03,  1.3397e-02, -5.8458e-03,\n",
       "                      -4.4659e-04, -1.8903e-02,  5.1883e-03,  7.0737e-03,  9.2972e-03,\n",
       "                      -1.9580e-03, -6.2209e-03, -5.8860e-04, -5.7408e-04,  5.2104e-03,\n",
       "                      -3.7931e-03,  2.0444e-03,  1.1756e-02,  6.6673e-03, -1.6055e-03],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.linear1.weight',\n",
       "              tensor([[ 0.1340, -0.0688, -0.0306,  ..., -0.0538,  0.1226, -0.1633],\n",
       "                      [-0.0007,  0.1936, -0.1005,  ..., -0.0480,  0.1100,  0.0528],\n",
       "                      [-0.0241,  0.1092, -0.1429,  ...,  0.0114,  0.1418,  0.1017],\n",
       "                      ...,\n",
       "                      [ 0.1010, -0.0095, -0.1981,  ..., -0.0680,  0.0828,  0.0780],\n",
       "                      [-0.1645,  0.1563,  0.1012,  ..., -0.1345, -0.1617, -0.0193],\n",
       "                      [-0.0600, -0.1749, -0.0078,  ...,  0.1993, -0.1502, -0.1781]],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.linear1.bias',\n",
       "              tensor([-0.1821, -0.2060,  0.0836,  ...,  0.0862, -0.1413, -0.0040],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.linear2.weight',\n",
       "              tensor([[-0.0164, -0.0129,  0.0146,  ..., -0.0103, -0.0100, -0.0030],\n",
       "                      [-0.0164,  0.0107,  0.0199,  ..., -0.0146, -0.0474,  0.0132],\n",
       "                      [-0.0177,  0.0080, -0.0011,  ...,  0.0117,  0.0185, -0.0206],\n",
       "                      ...,\n",
       "                      [-0.0107, -0.0109,  0.0042,  ...,  0.0139,  0.0191,  0.0107],\n",
       "                      [ 0.0046,  0.0088, -0.0045,  ...,  0.0158, -0.0053,  0.0085],\n",
       "                      [ 0.0334, -0.0135, -0.0088,  ...,  0.0030,  0.0194,  0.0313]],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.linear2.bias',\n",
       "              tensor([-0.0059,  0.0127,  0.0119, -0.0127,  0.0068,  0.0026,  0.0190,  0.0070,\n",
       "                       0.0112,  0.0038,  0.0006,  0.0050,  0.0088,  0.0063, -0.0224, -0.0009,\n",
       "                       0.0245,  0.0043,  0.0156, -0.0127,  0.0202, -0.0029,  0.0169, -0.0054,\n",
       "                      -0.0093], device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.norm1.weight',\n",
       "              tensor([1.0587, 0.9758, 0.9904, 1.0172, 0.9725, 1.0014, 1.0094, 1.0011, 0.9867,\n",
       "                      0.9940, 1.0055, 0.9938, 0.9613, 0.9510, 0.9864, 1.0007, 0.9939, 0.9712,\n",
       "                      0.9563, 0.9960, 1.0030, 0.9864, 1.0436, 1.0419, 0.9857],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.norm1.bias',\n",
       "              tensor([ 2.0963e-03, -3.6990e-05, -8.5892e-03,  9.3208e-04,  1.0591e-04,\n",
       "                       1.8404e-02,  2.1751e-02, -2.7551e-03,  6.8531e-03, -1.8708e-02,\n",
       "                       1.8413e-02,  1.5046e-02,  1.7871e-02, -3.2592e-03,  1.8031e-02,\n",
       "                      -1.2746e-02, -2.1229e-02,  1.5715e-02,  6.1159e-04, -1.7998e-02,\n",
       "                      -8.1775e-03,  8.3355e-03,  5.1439e-03, -5.3232e-03, -1.1678e-02],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.norm2.weight',\n",
       "              tensor([1.1705, 0.9681, 1.0726, 1.0595, 1.0559, 1.1032, 1.1219, 0.9037, 1.0827,\n",
       "                      0.9970, 1.0563, 1.1056, 0.9698, 1.0404, 0.9022, 1.0261, 0.9545, 0.9530,\n",
       "                      0.9762, 1.0496, 0.8534, 1.0158, 1.1869, 1.1748, 1.0254],\n",
       "                     device='cuda:0')),\n",
       "             ('transformer_encoder.layers.1.norm2.bias',\n",
       "              tensor([ 0.0039, -0.0467,  0.0222, -0.0215,  0.0195,  0.0432,  0.0370,  0.0235,\n",
       "                       0.0289, -0.0390,  0.0650,  0.0413,  0.0225, -0.0306,  0.0530,  0.0230,\n",
       "                      -0.0627,  0.0578, -0.0140, -0.0864, -0.0790,  0.0331,  0.0077, -0.0486,\n",
       "                      -0.0056], device='cuda:0')),\n",
       "             ('fc.1.weight',\n",
       "              tensor([[ 5.8350e-03,  1.3740e-02, -1.6852e-02,  ..., -7.7923e-03,\n",
       "                       -4.8946e-04, -1.9009e-02],\n",
       "                      [ 8.8029e-04,  2.4127e-03,  1.8579e-03,  ..., -1.7863e-02,\n",
       "                        6.1870e-05,  2.7813e-03],\n",
       "                      [-2.0373e-02,  1.8116e-02, -9.5748e-03,  ...,  3.5798e-02,\n",
       "                       -2.8888e-02, -4.0638e-02],\n",
       "                      ...,\n",
       "                      [ 9.4462e-03, -2.2757e-02, -4.6436e-03,  ..., -5.5085e-02,\n",
       "                        6.0861e-02,  2.2385e-02],\n",
       "                      [-2.4955e-02,  3.5864e-02,  2.8822e-02,  ..., -1.7856e-02,\n",
       "                       -2.5617e-02, -5.3739e-03],\n",
       "                      [ 1.6319e-02, -1.3927e-02, -9.4571e-03,  ..., -9.9128e-04,\n",
       "                        1.2539e-02,  4.4821e-03]], device='cuda:0')),\n",
       "             ('fc.1.bias',\n",
       "              tensor([-0.0362, -0.0110, -0.0230, -0.0249, -0.0210, -0.0254, -0.0088, -0.0320,\n",
       "                      -0.0134, -0.0088,  0.0025,  0.0098, -0.0103, -0.0016, -0.0136, -0.0355,\n",
       "                       0.0143, -0.0071,  0.0052, -0.0196, -0.0091,  0.0010, -0.0002,  0.0064,\n",
       "                      -0.0295, -0.0283, -0.0131,  0.0034, -0.0250, -0.0054, -0.0194, -0.0196,\n",
       "                      -0.0323, -0.0200, -0.0265, -0.0267, -0.0189, -0.0298, -0.0163, -0.0296,\n",
       "                      -0.0371, -0.0195,  0.0151,  0.0109, -0.0168, -0.0426, -0.0126, -0.0039,\n",
       "                      -0.0188,  0.0010], device='cuda:0')),\n",
       "             ('fc.4.weight',\n",
       "              tensor([[-0.0355,  0.1091, -0.1072,  0.0488, -0.0134, -0.0503,  0.0171, -0.0046,\n",
       "                       -0.1072,  0.0477, -0.0025,  0.1045, -0.0971,  0.1151, -0.1196, -0.0664,\n",
       "                        0.0856,  0.0333,  0.0084,  0.0045,  0.0812,  0.0832,  0.1562,  0.0094,\n",
       "                       -0.0947, -0.0707,  0.0008,  0.1717, -0.0267,  0.1475, -0.0817,  0.1417,\n",
       "                       -0.0076,  0.0195, -0.0462, -0.1044, -0.0975, -0.1288, -0.1096,  0.0752,\n",
       "                       -0.0462,  0.0284,  0.0746,  0.0219, -0.1173, -0.0484, -0.1426, -0.0222,\n",
       "                       -0.1332,  0.0113],\n",
       "                      [ 0.1154, -0.0979, -0.0250, -0.0408, -0.1038,  0.0802,  0.1138, -0.1138,\n",
       "                       -0.0138,  0.1580, -0.1628, -0.0835,  0.0553, -0.1246, -0.0135, -0.1489,\n",
       "                       -0.0548,  0.0811, -0.1604, -0.1322, -0.0941, -0.1233, -0.0934, -0.1722,\n",
       "                        0.0211,  0.0639,  0.1067, -0.1001, -0.1244, -0.1550,  0.0136, -0.1197,\n",
       "                        0.0777, -0.0785,  0.0832,  0.0101,  0.0336, -0.0317, -0.0179,  0.1307,\n",
       "                        0.0231, -0.0879, -0.1464, -0.1506,  0.0004,  0.0198, -0.0364, -0.1706,\n",
       "                       -0.0339, -0.1285]], device='cuda:0')),\n",
       "             ('fc.4.bias', tensor([-0.0504, -0.0085], device='cuda:0'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_robustness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
