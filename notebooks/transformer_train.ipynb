{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import wandb\n",
    "from ignite.contrib.handlers import wandb_logger\n",
    "from ignite.engine import (Engine, Events, create_supervised_evaluator,\n",
    "                           create_supervised_trainer)\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.handlers.param_scheduler import LRScheduler\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.datasets import FordDataset\n",
    "from src.models import TransformerClassification\n",
    "from src.utils import build_optimizer, str2torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../configs/transformer_87.json') as f:\n",
    "    config =  json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/FordA/FordA_TRAIN.arff\"\n",
    "test_path = \"../data/FordA/FordA_TEST.arff\"\n",
    "\n",
    "train_dataset = FordDataset(train_path, config['data'])\n",
    "test_dataset = FordDataset(test_path, config['data'])\n",
    "\n",
    "idx = np.arange(len(train_dataset))\n",
    "idx_train, idx_val = train_test_split(idx, train_size=0.8, stratify=train_dataset.labels, random_state=config['random_state'])\n",
    "\n",
    "train_sampler = SubsetRandomSampler(idx)\n",
    "val_sampler = SubsetRandomSampler(idx_val)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['data']['batch_size'], sampler=train_sampler)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=config['data']['batch_size'], sampler=val_sampler)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:01m9lhtm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-dawn-226</strong> at: <a href='https://wandb.ai/ts-robustness/ml-course/runs/01m9lhtm' target=\"_blank\">https://wandb.ai/ts-robustness/ml-course/runs/01m9lhtm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240303_221021-01m9lhtm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:01m9lhtm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ptmeg\\OneDrive\\Документы\\Skoltech\\Term3\\ML\\ts_robustness\\notebooks\\wandb\\run-20240303_221220-tcp4yitv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ts-robustness/ml-course/runs/tcp4yitv' target=\"_blank\">visionary-deluge-227</a></strong> to <a href='https://wandb.ai/ts-robustness/ml-course' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ts-robustness/ml-course' target=\"_blank\">https://wandb.ai/ts-robustness/ml-course</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ts-robustness/ml-course/runs/tcp4yitv' target=\"_blank\">https://wandb.ai/ts-robustness/ml-course/runs/tcp4yitv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results - Epoch: 1  Avg accuracy: 0.69 Avg loss: 0.6025\n",
      "Validation Results - Epoch: 1  Avg accuracy: 0.69 Avg loss: 0.6019\n",
      "Training Results - Epoch: 2  Avg accuracy: 0.81 Avg loss: 0.4264\n",
      "Validation Results - Epoch: 2  Avg accuracy: 0.81 Avg loss: 0.4233\n",
      "Training Results - Epoch: 3  Avg accuracy: 0.84 Avg loss: 0.3639\n",
      "Validation Results - Epoch: 3  Avg accuracy: 0.84 Avg loss: 0.3572\n",
      "Training Results - Epoch: 4  Avg accuracy: 0.85 Avg loss: 0.3424\n",
      "Validation Results - Epoch: 4  Avg accuracy: 0.86 Avg loss: 0.3341\n",
      "Training Results - Epoch: 5  Avg accuracy: 0.86 Avg loss: 0.3168\n",
      "Validation Results - Epoch: 5  Avg accuracy: 0.86 Avg loss: 0.3065\n",
      "Training Results - Epoch: 6  Avg accuracy: 0.87 Avg loss: 0.3005\n",
      "Validation Results - Epoch: 6  Avg accuracy: 0.88 Avg loss: 0.2894\n",
      "Training Results - Epoch: 7  Avg accuracy: 0.88 Avg loss: 0.2959\n",
      "Validation Results - Epoch: 7  Avg accuracy: 0.89 Avg loss: 0.2854\n",
      "Training Results - Epoch: 8  Avg accuracy: 0.87 Avg loss: 0.2964\n",
      "Validation Results - Epoch: 8  Avg accuracy: 0.88 Avg loss: 0.2814\n",
      "Training Results - Epoch: 9  Avg accuracy: 0.88 Avg loss: 0.2920\n",
      "Validation Results - Epoch: 9  Avg accuracy: 0.89 Avg loss: 0.2729\n",
      "Training Results - Epoch: 10  Avg accuracy: 0.89 Avg loss: 0.2710\n",
      "Validation Results - Epoch: 10  Avg accuracy: 0.90 Avg loss: 0.2543\n",
      "Training Results - Epoch: 11  Avg accuracy: 0.88 Avg loss: 0.2752\n",
      "Validation Results - Epoch: 11  Avg accuracy: 0.90 Avg loss: 0.2576\n",
      "Training Results - Epoch: 12  Avg accuracy: 0.88 Avg loss: 0.2821\n",
      "Validation Results - Epoch: 12  Avg accuracy: 0.89 Avg loss: 0.2691\n",
      "Training Results - Epoch: 13  Avg accuracy: 0.89 Avg loss: 0.2669\n",
      "Validation Results - Epoch: 13  Avg accuracy: 0.90 Avg loss: 0.2502\n",
      "Training Results - Epoch: 14  Avg accuracy: 0.88 Avg loss: 0.2784\n",
      "Validation Results - Epoch: 14  Avg accuracy: 0.89 Avg loss: 0.2665\n",
      "Training Results - Epoch: 15  Avg accuracy: 0.89 Avg loss: 0.2685\n",
      "Validation Results - Epoch: 15  Avg accuracy: 0.89 Avg loss: 0.2550\n",
      "Training Results - Epoch: 16  Avg accuracy: 0.89 Avg loss: 0.2642\n",
      "Validation Results - Epoch: 16  Avg accuracy: 0.90 Avg loss: 0.2506\n",
      "Training Results - Epoch: 17  Avg accuracy: 0.89 Avg loss: 0.2555\n",
      "Validation Results - Epoch: 17  Avg accuracy: 0.90 Avg loss: 0.2391\n",
      "Training Results - Epoch: 18  Avg accuracy: 0.90 Avg loss: 0.2462\n",
      "Validation Results - Epoch: 18  Avg accuracy: 0.90 Avg loss: 0.2312\n",
      "Training Results - Epoch: 19  Avg accuracy: 0.90 Avg loss: 0.2482\n",
      "Validation Results - Epoch: 19  Avg accuracy: 0.91 Avg loss: 0.2328\n",
      "Training Results - Epoch: 20  Avg accuracy: 0.90 Avg loss: 0.2462\n",
      "Validation Results - Epoch: 20  Avg accuracy: 0.91 Avg loss: 0.2333\n",
      "Training Results - Epoch: 21  Avg accuracy: 0.89 Avg loss: 0.2592\n",
      "Validation Results - Epoch: 21  Avg accuracy: 0.89 Avg loss: 0.2450\n",
      "Training Results - Epoch: 22  Avg accuracy: 0.90 Avg loss: 0.2435\n",
      "Validation Results - Epoch: 22  Avg accuracy: 0.90 Avg loss: 0.2291\n",
      "Training Results - Epoch: 23  Avg accuracy: 0.89 Avg loss: 0.2535\n",
      "Validation Results - Epoch: 23  Avg accuracy: 0.90 Avg loss: 0.2366\n",
      "Training Results - Epoch: 24  Avg accuracy: 0.89 Avg loss: 0.2530\n",
      "Validation Results - Epoch: 24  Avg accuracy: 0.90 Avg loss: 0.2385\n",
      "Training Results - Epoch: 25  Avg accuracy: 0.90 Avg loss: 0.2320\n",
      "Validation Results - Epoch: 25  Avg accuracy: 0.91 Avg loss: 0.2170\n",
      "Training Results - Epoch: 26  Avg accuracy: 0.91 Avg loss: 0.2280\n",
      "Validation Results - Epoch: 26  Avg accuracy: 0.91 Avg loss: 0.2109\n",
      "Training Results - Epoch: 27  Avg accuracy: 0.91 Avg loss: 0.2237\n",
      "Validation Results - Epoch: 27  Avg accuracy: 0.91 Avg loss: 0.2081\n",
      "Training Results - Epoch: 28  Avg accuracy: 0.90 Avg loss: 0.2350\n",
      "Validation Results - Epoch: 28  Avg accuracy: 0.91 Avg loss: 0.2225\n",
      "Training Results - Epoch: 29  Avg accuracy: 0.89 Avg loss: 0.2480\n",
      "Validation Results - Epoch: 29  Avg accuracy: 0.90 Avg loss: 0.2341\n",
      "Training Results - Epoch: 30  Avg accuracy: 0.90 Avg loss: 0.2356\n",
      "Validation Results - Epoch: 30  Avg accuracy: 0.91 Avg loss: 0.2219\n",
      "Test Results - Epoch: 30  Avg accuracy: 0.87 Avg loss: 0.3198\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███▇█▇███████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇████▇█▇▇████▇█</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8697</td></tr><tr><td>test_loss</td><td>0.31976</td></tr><tr><td>train_accuracy</td><td>0.90107</td></tr><tr><td>train_loss</td><td>0.23561</td></tr><tr><td>val_accuracy</td><td>0.90715</td></tr><tr><td>val_loss</td><td>0.22194</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-deluge-227</strong> at: <a href='https://wandb.ai/ts-robustness/ml-course/runs/tcp4yitv' target=\"_blank\">https://wandb.ai/ts-robustness/ml-course/runs/tcp4yitv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240303_221220-tcp4yitv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize your model\n",
    "wandb.init(entity='ts-robustness', project='ml-course', config=config, tags=['hypersearch'])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config['train']['optimizer'] = str2torch(config['train']['optimizer'])\n",
    "\n",
    "model = TransformerClassification(config).to(device)\n",
    "\n",
    "# Initialize your optimizer and criterion\n",
    "optimizer = build_optimizer(config, model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch[0].to(device), batch[1].to(device)\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y.long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        y_pred = model(x)\n",
    "        return y_pred, y\n",
    "\n",
    "train_evaluator = Engine(validation_step)\n",
    "val_evaluator = Engine(validation_step)\n",
    "test_evaluator = Engine(validation_step)\n",
    "\n",
    "# Attach metrics to the evaluators\n",
    "metrics = {\n",
    "    'accuracy': Accuracy(output_transform=lambda x: (torch.argmax(x[0], dim=1), x[1])),\n",
    "    'loss': Loss(criterion, output_transform=lambda x: (x[0], x[1].long()))\n",
    "}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(train_evaluator, name)\n",
    "    metric.attach(val_evaluator, name)\n",
    "    metric.attach(test_evaluator, name)\n",
    "\n",
    "\n",
    "# checkpoint_handler = ModelCheckpoint(dirname='saved_models', filename_prefix='best',\n",
    "#                                      n_saved=1, require_empty=False,\n",
    "#                                      score_function=lambda engine: engine.state.metrics['accuracy'],\n",
    "#                                      score_name=\"accuracy\", global_step_transform=lambda *_: trainer.state.epoch)\n",
    "# val_evaluator.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {\"model\": model})\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    train_evaluator.run(train_dataloader)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    print(\"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.4f}\"\n",
    "          .format(trainer.state.epoch, metrics['accuracy'], metrics['loss']))\n",
    "    wandb.log({\"train_accuracy\": metrics['accuracy'],\n",
    "               \"train_loss\": metrics['loss']})\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    val_evaluator.run(val_dataloader)\n",
    "    metrics = val_evaluator.state.metrics\n",
    "    print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.4f}\"\n",
    "          .format(trainer.state.epoch, metrics['accuracy'], metrics['loss']))\n",
    "    wandb.log({\"val_accuracy\": metrics['accuracy'],\n",
    "               \"val_loss\": metrics['loss']})\n",
    "    \n",
    "@trainer.on(Events.COMPLETED)\n",
    "def log_test_results(trainer):\n",
    "    test_evaluator.run(test_dataloader)\n",
    "    metrics = test_evaluator.state.metrics\n",
    "    print(\"Test Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.4f}\"\n",
    "          .format(trainer.state.epoch, metrics['accuracy'], metrics['loss']))\n",
    "    wandb.log({\"test_accuracy\": metrics['accuracy'],\n",
    "               \"test_loss\": metrics['loss']})\n",
    "\n",
    "\n",
    "# Run the training loop\n",
    "trainer.run(train_dataloader, max_epochs=config['train']['n_epoch'])\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.to('cpu').state_dict(), \"../models/trans_2outp_87.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in transformer_encoder: 214346\n",
      "Number of parameters in fc layer: 187652\n"
     ]
    }
   ],
   "source": [
    "# Count the number of parameters in the transformer_encoder layer\n",
    "transformer_encoder_params = sum(p.numel() for p in model.transformer_encoder.parameters())\n",
    "\n",
    "# Count the number of parameters in the fc layer\n",
    "fc_params = sum(p.numel() for p in model.fc.parameters())\n",
    "\n",
    "print(\"Number of parameters in transformer_encoder:\", transformer_encoder_params)\n",
    "print(\"Number of parameters in fc layer:\", fc_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_robustness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
