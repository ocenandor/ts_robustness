{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import wandb\n",
    "from ignite.contrib.handlers import wandb_logger\n",
    "from ignite.engine import (Engine, Events, create_supervised_evaluator,\n",
    "                           create_supervised_trainer)\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.handlers.param_scheduler import LRScheduler\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.datasets import make_dataset\n",
    "from src.models import TransformerClassification\n",
    "from src.utils import build_optimizer, str2torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../configs/transformer_87.json') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = make_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 150])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgamma_function\u001b[0m (\u001b[33mts-robustness\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd758ad328e468bada91aef063dbd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Study\\Skoltech\\MachineLearning\\ts_robustness\\notebooks\\wandb\\run-20240312_182239-oand93c7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ts-robustness/ml-course/runs/oand93c7' target=\"_blank\">olive-wind-304</a></strong> to <a href='https://wandb.ai/ts-robustness/ml-course' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ts-robustness/ml-course' target=\"_blank\">https://wandb.ai/ts-robustness/ml-course</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ts-robustness/ml-course/runs/oand93c7' target=\"_blank\">https://wandb.ai/ts-robustness/ml-course/runs/oand93c7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results - Epoch: 1  Avg accuracy: 0.57 Avg loss: 0.6739\n",
      "Validation Results - Epoch: 1  Avg accuracy: 0.57 Avg loss: 0.6741\n",
      "Test Results - Epoch: 1  Avg accuracy: 0.56 Avg loss: 0.6751\n",
      "Training Results - Epoch: 2  Avg accuracy: 0.60 Avg loss: 0.6541\n",
      "Validation Results - Epoch: 2  Avg accuracy: 0.60 Avg loss: 0.6547\n",
      "Test Results - Epoch: 2  Avg accuracy: 0.59 Avg loss: 0.6594\n",
      "Training Results - Epoch: 3  Avg accuracy: 0.67 Avg loss: 0.6230\n",
      "Validation Results - Epoch: 3  Avg accuracy: 0.67 Avg loss: 0.6213\n",
      "Test Results - Epoch: 3  Avg accuracy: 0.64 Avg loss: 0.6336\n",
      "Training Results - Epoch: 4  Avg accuracy: 0.70 Avg loss: 0.5788\n",
      "Validation Results - Epoch: 4  Avg accuracy: 0.70 Avg loss: 0.5776\n",
      "Test Results - Epoch: 4  Avg accuracy: 0.69 Avg loss: 0.5882\n",
      "Training Results - Epoch: 5  Avg accuracy: 0.75 Avg loss: 0.5098\n",
      "Validation Results - Epoch: 5  Avg accuracy: 0.75 Avg loss: 0.5083\n",
      "Test Results - Epoch: 5  Avg accuracy: 0.75 Avg loss: 0.5125\n",
      "Training Results - Epoch: 6  Avg accuracy: 0.80 Avg loss: 0.4134\n",
      "Validation Results - Epoch: 6  Avg accuracy: 0.80 Avg loss: 0.4052\n",
      "Test Results - Epoch: 6  Avg accuracy: 0.79 Avg loss: 0.4209\n",
      "Training Results - Epoch: 7  Avg accuracy: 0.82 Avg loss: 0.3754\n",
      "Validation Results - Epoch: 7  Avg accuracy: 0.82 Avg loss: 0.3673\n",
      "Test Results - Epoch: 7  Avg accuracy: 0.82 Avg loss: 0.3824\n",
      "Training Results - Epoch: 8  Avg accuracy: 0.83 Avg loss: 0.3604\n",
      "Validation Results - Epoch: 8  Avg accuracy: 0.83 Avg loss: 0.3466\n",
      "Test Results - Epoch: 8  Avg accuracy: 0.82 Avg loss: 0.3742\n",
      "Training Results - Epoch: 9  Avg accuracy: 0.84 Avg loss: 0.3389\n",
      "Validation Results - Epoch: 9  Avg accuracy: 0.85 Avg loss: 0.3241\n",
      "Test Results - Epoch: 9  Avg accuracy: 0.84 Avg loss: 0.3568\n",
      "Training Results - Epoch: 10  Avg accuracy: 0.87 Avg loss: 0.3155\n",
      "Validation Results - Epoch: 10  Avg accuracy: 0.88 Avg loss: 0.3031\n",
      "Test Results - Epoch: 10  Avg accuracy: 0.86 Avg loss: 0.3345\n",
      "Training Results - Epoch: 11  Avg accuracy: 0.86 Avg loss: 0.3075\n",
      "Validation Results - Epoch: 11  Avg accuracy: 0.87 Avg loss: 0.2891\n",
      "Test Results - Epoch: 11  Avg accuracy: 0.86 Avg loss: 0.3343\n",
      "Training Results - Epoch: 12  Avg accuracy: 0.87 Avg loss: 0.2980\n",
      "Validation Results - Epoch: 12  Avg accuracy: 0.89 Avg loss: 0.2793\n",
      "Test Results - Epoch: 12  Avg accuracy: 0.87 Avg loss: 0.3215\n",
      "Training Results - Epoch: 13  Avg accuracy: 0.85 Avg loss: 0.3406\n",
      "Validation Results - Epoch: 13  Avg accuracy: 0.85 Avg loss: 0.3211\n",
      "Test Results - Epoch: 13  Avg accuracy: 0.84 Avg loss: 0.3676\n",
      "Training Results - Epoch: 14  Avg accuracy: 0.87 Avg loss: 0.3073\n",
      "Validation Results - Epoch: 14  Avg accuracy: 0.87 Avg loss: 0.2889\n",
      "Test Results - Epoch: 14  Avg accuracy: 0.86 Avg loss: 0.3374\n",
      "Training Results - Epoch: 15  Avg accuracy: 0.88 Avg loss: 0.2796\n",
      "Validation Results - Epoch: 15  Avg accuracy: 0.89 Avg loss: 0.2628\n",
      "Test Results - Epoch: 15  Avg accuracy: 0.87 Avg loss: 0.3087\n",
      "Training Results - Epoch: 16  Avg accuracy: 0.87 Avg loss: 0.2911\n",
      "Validation Results - Epoch: 16  Avg accuracy: 0.88 Avg loss: 0.2698\n",
      "Test Results - Epoch: 16  Avg accuracy: 0.86 Avg loss: 0.3260\n",
      "Training Results - Epoch: 17  Avg accuracy: 0.87 Avg loss: 0.3051\n",
      "Validation Results - Epoch: 17  Avg accuracy: 0.88 Avg loss: 0.2814\n",
      "Test Results - Epoch: 17  Avg accuracy: 0.86 Avg loss: 0.3406\n",
      "Training Results - Epoch: 18  Avg accuracy: 0.88 Avg loss: 0.2744\n",
      "Validation Results - Epoch: 18  Avg accuracy: 0.90 Avg loss: 0.2536\n",
      "Test Results - Epoch: 18  Avg accuracy: 0.87 Avg loss: 0.3134\n",
      "Training Results - Epoch: 19  Avg accuracy: 0.89 Avg loss: 0.2696\n",
      "Validation Results - Epoch: 19  Avg accuracy: 0.90 Avg loss: 0.2480\n",
      "Test Results - Epoch: 19  Avg accuracy: 0.88 Avg loss: 0.3073\n",
      "Training Results - Epoch: 20  Avg accuracy: 0.88 Avg loss: 0.2776\n",
      "Validation Results - Epoch: 20  Avg accuracy: 0.89 Avg loss: 0.2519\n",
      "Test Results - Epoch: 20  Avg accuracy: 0.87 Avg loss: 0.3193\n",
      "Training Results - Epoch: 21  Avg accuracy: 0.89 Avg loss: 0.2608\n",
      "Validation Results - Epoch: 21  Avg accuracy: 0.90 Avg loss: 0.2394\n",
      "Test Results - Epoch: 21  Avg accuracy: 0.88 Avg loss: 0.3028\n",
      "Training Results - Epoch: 22  Avg accuracy: 0.89 Avg loss: 0.2637\n",
      "Validation Results - Epoch: 22  Avg accuracy: 0.90 Avg loss: 0.2398\n",
      "Test Results - Epoch: 22  Avg accuracy: 0.88 Avg loss: 0.3084\n",
      "Training Results - Epoch: 23  Avg accuracy: 0.89 Avg loss: 0.2715\n",
      "Validation Results - Epoch: 23  Avg accuracy: 0.89 Avg loss: 0.2483\n",
      "Test Results - Epoch: 23  Avg accuracy: 0.87 Avg loss: 0.3197\n",
      "Training Results - Epoch: 24  Avg accuracy: 0.89 Avg loss: 0.2598\n",
      "Validation Results - Epoch: 24  Avg accuracy: 0.90 Avg loss: 0.2380\n",
      "Test Results - Epoch: 24  Avg accuracy: 0.88 Avg loss: 0.3082\n",
      "Training Results - Epoch: 25  Avg accuracy: 0.89 Avg loss: 0.2534\n",
      "Validation Results - Epoch: 25  Avg accuracy: 0.91 Avg loss: 0.2322\n",
      "Test Results - Epoch: 25  Avg accuracy: 0.88 Avg loss: 0.3017\n",
      "Training Results - Epoch: 26  Avg accuracy: 0.90 Avg loss: 0.2485\n",
      "Validation Results - Epoch: 26  Avg accuracy: 0.91 Avg loss: 0.2294\n",
      "Test Results - Epoch: 26  Avg accuracy: 0.88 Avg loss: 0.2977\n",
      "Training Results - Epoch: 27  Avg accuracy: 0.89 Avg loss: 0.2602\n",
      "Validation Results - Epoch: 27  Avg accuracy: 0.90 Avg loss: 0.2392\n",
      "Test Results - Epoch: 27  Avg accuracy: 0.87 Avg loss: 0.3185\n",
      "Training Results - Epoch: 28  Avg accuracy: 0.88 Avg loss: 0.2737\n",
      "Validation Results - Epoch: 28  Avg accuracy: 0.89 Avg loss: 0.2514\n",
      "Test Results - Epoch: 28  Avg accuracy: 0.87 Avg loss: 0.3345\n",
      "Training Results - Epoch: 29  Avg accuracy: 0.90 Avg loss: 0.2388\n",
      "Validation Results - Epoch: 29  Avg accuracy: 0.91 Avg loss: 0.2201\n",
      "Test Results - Epoch: 29  Avg accuracy: 0.88 Avg loss: 0.2984\n",
      "Training Results - Epoch: 30  Avg accuracy: 0.88 Avg loss: 0.2725\n",
      "Validation Results - Epoch: 30  Avg accuracy: 0.89 Avg loss: 0.2543\n",
      "Test Results - Epoch: 30  Avg accuracy: 0.86 Avg loss: 0.3336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 3390\n",
       "\tepoch: 30\n",
       "\tepoch_length: 113\n",
       "\tmax_epochs: 30\n",
       "\toutput: 0.3266116678714752\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize your model\n",
    "wandb.init(entity='ts-robustness', project='ml-course', config=config, tags=['hypersearch'])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config['train']['optimizer'] = str2torch(config['train']['optimizer'])\n",
    "\n",
    "model = TransformerClassification(config).to(device)\n",
    "\n",
    "# Initialize your optimizer and criterion\n",
    "optimizer = build_optimizer(config, model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch[0].to(device), batch[1].to(device)\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y.long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        y_pred = model(x)\n",
    "        return y_pred, y\n",
    "\n",
    "train_evaluator = Engine(validation_step)\n",
    "val_evaluator = Engine(validation_step)\n",
    "test_evaluator = Engine(validation_step)\n",
    "\n",
    "# Attach metrics to the evaluators\n",
    "metrics = {\n",
    "    'accuracy': Accuracy(output_transform=lambda x: (torch.argmax(x[0], dim=1), x[1])),\n",
    "    'loss': Loss(criterion, output_transform=lambda x: (x[0], x[1].long()))\n",
    "}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(train_evaluator, name)\n",
    "    metric.attach(val_evaluator, name)\n",
    "    metric.attach(test_evaluator, name)\n",
    "\n",
    "\n",
    "checkpoint_handler = ModelCheckpoint(dirname=wandb.run.dir + '/../saved_models', filename_prefix='best',\n",
    "                                     n_saved=1, require_empty=False,\n",
    "                                     score_function=lambda engine: engine.state.metrics['accuracy'],\n",
    "                                     score_name=\"accuracy\", global_step_transform=lambda *_: trainer.state.epoch)\n",
    "test_evaluator.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {\"model\": model})\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    train_evaluator.run(train_dataloader)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    print(\"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.4f}\"\n",
    "          .format(trainer.state.epoch, metrics['accuracy'], metrics['loss']))\n",
    "    wandb.log({\"train_accuracy\": metrics['accuracy'],\n",
    "               \"train_loss\": metrics['loss']})\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    val_evaluator.run(val_dataloader)\n",
    "    metrics = val_evaluator.state.metrics\n",
    "    print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.4f}\"\n",
    "          .format(trainer.state.epoch, metrics['accuracy'], metrics['loss']))\n",
    "    wandb.log({\"val_accuracy\": metrics['accuracy'],\n",
    "               \"val_loss\": metrics['loss']})\n",
    "    \n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_test_results(trainer):\n",
    "    test_evaluator.run(test_dataloader)\n",
    "    metrics = test_evaluator.state.metrics\n",
    "    print(\"Test Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.4f}\"\n",
    "          .format(trainer.state.epoch, metrics['accuracy'], metrics['loss']))\n",
    "    wandb.log({\"test_accuracy\": metrics['accuracy'],\n",
    "               \"test_loss\": metrics['loss']})\n",
    "\n",
    "\n",
    "# Run the training loop\n",
    "trainer.run(train_dataloader, max_epochs=config['train']['n_epoch'])\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in transformer_encoder: 214346\n",
      "Number of parameters in fc layer: 187652\n"
     ]
    }
   ],
   "source": [
    "# Count the number of parameters in the transformer_encoder layer\n",
    "transformer_encoder_params = sum(p.numel() for p in model.transformer_encoder.parameters())\n",
    "\n",
    "# Count the number of parameters in the fc layer\n",
    "fc_params = sum(p.numel() for p in model.fc.parameters())\n",
    "\n",
    "print(\"Number of parameters in transformer_encoder:\", transformer_encoder_params)\n",
    "print(\"Number of parameters in fc layer:\", fc_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req_grad(model: nn.Module, state: bool = True) -> None:\n",
    "    \"\"\"Set requires_grad of all model parameters to the desired value.\n",
    "\n",
    "    :param model: the model\n",
    "    :param state: desired value for requires_grad\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BMI_attack(num_iterations):\n",
    "    epsilon = 0.01\n",
    "    req_grad(model, state=True)\n",
    "    model.train()\n",
    "    loss_function = torch.nn.BCELoss()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for input, target in test_dataloader:\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        adversarial_input = input.clone().requires_grad_(True)  \n",
    "        \n",
    "        \n",
    "        for _ in range(num_iterations):\n",
    "            adversarial_input.requires_grad = True\n",
    "            #print(adversarial_input.requires_grad)\n",
    "            predictions = model(adversarial_input)\n",
    "            loss = loss_function(predictions.flatten(), target)\n",
    "            \n",
    "            grad_ = torch.autograd.grad(loss, adversarial_input, retain_graph=True)[0]\n",
    "            adversarial_input = adversarial_input.data + epsilon * torch.sign(grad_)\n",
    "    \n",
    "        adversarial_predictions = model(adversarial_input)\n",
    "        correct += (adversarial_predictions.to('cpu').round().reshape(1,-1) == target.to('cpu')).sum().item()\n",
    "        total += len(target)\n",
    "    return correct/total, input, adversarial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([256])) that is different to the input size (torch.Size([512])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acc, inp, adv_inp \u001b[38;5;241m=\u001b[39m BMI_attack(\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mBMI_attack\u001b[1;34m(num_iterations)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#print(adversarial_input.requires_grad)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(adversarial_input)\n\u001b[1;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(predictions\u001b[38;5;241m.\u001b[39mflatten(), target)\n\u001b[0;32m     22\u001b[0m grad_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(loss, adversarial_input, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     23\u001b[0m adversarial_input \u001b[38;5;241m=\u001b[39m adversarial_input\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m epsilon \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(grad_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3116\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3121\u001b[0m     )\n\u001b[0;32m   3123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([512])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "acc, inp, adv_inp = BMI_attack(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
